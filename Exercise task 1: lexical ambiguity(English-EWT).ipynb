{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0s8co3QunL+i04GTe+0EK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Introduction-to-Human-Language-Technology/blob/main/Exercise%20task%201%3A%20lexical%20ambiguity(English-EWT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "from collections import Counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_fs0RcCbumQ",
        "outputId": "f4af7ad9-e80b-4a28-9a99-bd1d55e63bb6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/simulate111/Introduction-to-Human-Language-Technology/main/en_ewt-ud-train.conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq1cE8mfOp6i",
        "outputId": "9a419936-dae9-4e3e-b3fa-33fbfc63a009"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-16 00:31:52--  https://raw.githubusercontent.com/simulate111/Introduction-to-Human-Language-Technology/main/en_ewt-ud-train.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13846707 (13M) [text/plain]\n",
            "Saving to: ‘en_ewt-ud-train.conllu.2’\n",
            "\n",
            "en_ewt-ud-train.con 100%[===================>]  13.21M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-03-16 00:31:52 (103 MB/s) - ‘en_ewt-ud-train.conllu.2’ saved [13846707/13846707]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data and tokenize\n",
        "with open(\"en_ewt-ud-train.conllu\", \"r\") as file:\n",
        "    data = file.read()\n",
        "\n",
        "lines = data.split(\"\\n\")\n",
        "tokens = []\n",
        "for line in lines:\n",
        "    parts = line.split(\"\\t\")\n",
        "    try:\n",
        "        tokens.append(parts[1])\n",
        "    except IndexError:\n",
        "        continue\n",
        "text = \" \".join(tokens)\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwdBeWz6Ot9c",
        "outputId": "804621e9-746e-4dad-dca0-0338025483cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "HP_Ag0UWoV6J",
        "outputId": "69d532ef-46e6-467a-bc6c-206fd5f4a317"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Al - Zaman : American forces killed Shaikh Abdullah al - Ani , the preacher at the mosque in the town of Qaim , near the Syrian border . [ This killing of a respected cleric will be causing us trouble for years to come . ] DPA : Iraqi authorities announced that they had busted up 3 terrorist cells operating in Baghdad . Two of them were being run by 2 officials of the Ministry of the Interior ! The MoI in Iraq is equivalent to the US FBI , so this would be like having J. Edgar Hoover unwittingly'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing some common unnecessary symbols and characters\n",
        "import re\n",
        "tokenized = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
        "tokenized = [re.sub(r\"(n't)\", r\" \\1\", token) for token in tokenized]\n",
        "tokenized = [token for token in tokenized if not re.match(r'\\d+', token)]\n",
        "tokenized = [line for line in tokenized if line.strip()]\n",
        "tokenized = [token for token in tokenized if len(token) > 1]\n",
        "for line in tokenized[:10]:\n",
        "    print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjoSTj6ePXkK",
        "outputId": "1c245c1c-6e82-4d75-8b1b-ad5c76cdd719"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Al\n",
            "Zaman\n",
            "American\n",
            "forces\n",
            "killed\n",
            "Shaikh\n",
            "Abdullah\n",
            "al\n",
            "Ani\n",
            "the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = len(tokenized)\n",
        "print('THe total number of words in the dataset')\n",
        "num_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRaGsqBdcGnt",
        "outputId": "bd84d654-779f-47ff-a456-e2cac44d2e32"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THe total number of words in the dataset\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "171448"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count word frequencies\n",
        "word_counts = Counter(tokenized)\n",
        "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "for word, count in sorted_word_counts[:10]:\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tsAn8xMgBLo",
        "outputId": "239cf83a-0326-4c9c-eb3f-c249754992e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the: 8151\n",
            "to: 5077\n",
            "and: 4855\n",
            "of: 3589\n",
            "in: 2911\n",
            "is: 2178\n",
            "you: 2029\n",
            "that: 1981\n",
            "for: 1789\n",
            "it: 1588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To identify common verbs and nouns, two dictionary of nouns and verbs are made and the common words in both dictionaries will be counted.\n",
        "Language model of en_core_web_sm in soacy library is used as facilator.**"
      ],
      "metadata": {
        "id": "uxnuJjp1bVZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "pattern = re.compile(r'[^\\w\\s]')\n",
        "chunks = text.split('.')\n",
        "chunks = [pattern.sub('', chunk) for chunk in chunks]\n",
        "noun_freq = {}\n",
        "verb_freq = {}\n",
        "\n",
        "for chunk in chunks:\n",
        "    doc = nlp(chunk)\n",
        "    for token in doc:\n",
        "        if len(token.text) == 1:\n",
        "            continue\n",
        "        if token.pos_ == 'NOUN':\n",
        "            noun_freq[token.text] = noun_freq.get(token.text, 0) + 1\n",
        "        elif token.pos_ == 'VERB':\n",
        "            verb_freq[token.text] = verb_freq.get(token.text, 0) + 1"
      ],
      "metadata": {
        "id": "haw2G-vyWuFJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of nouns:\", sum(noun_freq.values()))\n",
        "print(\"Total number of verbs:\", sum(verb_freq.values()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5pGJwCLd2Vx",
        "outputId": "d494c4e9-7a59-42a3-ab45-d9093a43a7a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of nouns: 32572\n",
            "Total number of verbs: 23010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_noun_freq = sorted(noun_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"Top 10 NOUN:\")\n",
        "for noun, freq in sorted_noun_freq[:10]:\n",
        "    print(f\"{noun}: {freq}\")\n",
        "\n",
        "sorted_verb_freq = sorted(verb_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nTop 10 VERB:\")\n",
        "for verb, freq in sorted_verb_freq[:10]:\n",
        "    print(f\"{verb}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0GZAw1OYDrT",
        "outputId": "74b6edee-fffe-463a-c0ce-be2d300d8f48"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 NOUN:\n",
            "time: 386\n",
            "people: 233\n",
            "way: 194\n",
            "place: 181\n",
            "year: 168\n",
            "day: 166\n",
            "food: 158\n",
            "service: 157\n",
            "years: 154\n",
            "Thanks: 147\n",
            "\n",
            "Top 10 VERB:\n",
            "have: 735\n",
            "get: 354\n",
            "know: 339\n",
            "had: 316\n",
            "do: 264\n",
            "go: 260\n",
            "said: 233\n",
            "want: 208\n",
            "going: 191\n",
            "see: 185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "common_words = set(noun_freq.keys()).intersection(set(verb_freq.keys()))\n",
        "print(\"Number of common words between nouns and verbs:\", len(common_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAmPSXp7YxgR",
        "outputId": "f989252a-e548-4524-c099-2f53ef26edec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of common words between nouns and verbs: 798\n"
          ]
        }
      ]
    }
  ]
}