{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMzqv2dNxT4GQeylOmtUbL4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Introduction-to-Human-Language-Technology/blob/main/Exercise%20task%201%3A%20lexical%20ambiguity(English-EWT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "nltk.download('punkt')\n",
        "from collections import Counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_fs0RcCbumQ",
        "outputId": "5e7465f6-f1ea-4754-dd40-1cfd2e5c6239"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/simulate111/Introduction-to-Human-Language-Technology/main/en_ewt-ud-train.conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq1cE8mfOp6i",
        "outputId": "79107cdf-bb55-4983-f80a-c66220641228"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-15 23:51:22--  https://raw.githubusercontent.com/simulate111/Introduction-to-Human-Language-Technology/main/en_ewt-ud-train.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13846707 (13M) [text/plain]\n",
            "Saving to: ‘en_ewt-ud-train.conllu.3’\n",
            "\n",
            "\ren_ewt-ud-train.con   0%[                    ]       0  --.-KB/s               \ren_ewt-ud-train.con 100%[===================>]  13.21M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-03-15 23:51:22 (121 MB/s) - ‘en_ewt-ud-train.conllu.3’ saved [13846707/13846707]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data and tokenize\n",
        "with open(\"en_ewt-ud-train.conllu\", \"r\") as file:\n",
        "    data = file.read()\n",
        "\n",
        "lines = data.split(\"\\n\")\n",
        "tokens = []\n",
        "for line in lines:\n",
        "    parts = line.split(\"\\t\")\n",
        "    try:\n",
        "        tokens.append(parts[1])\n",
        "    except IndexError:\n",
        "        continue\n",
        "text = \" \".join(tokens)\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwdBeWz6Ot9c",
        "outputId": "bcb3f686-1381-4748-ff64-daad62dff37a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing some common unnecessary symbols and characters\n",
        "import re\n",
        "tokenized = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
        "tokenized = [re.sub(r\"(n't)\", r\" \\1\", token) for token in tokenized]\n",
        "tokenized = [token for token in tokenized if not re.match(r'\\d+', token)]\n",
        "tokenized = [line for line in tokenized if line.strip()]\n",
        "tokenized = [token for token in tokenized if len(token) > 1]\n",
        "for line in tokenized[:10]:\n",
        "    print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjoSTj6ePXkK",
        "outputId": "e1ff766c-cea4-4584-bd6d-87e2e4252acc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Al\n",
            "Zaman\n",
            "American\n",
            "forces\n",
            "killed\n",
            "Shaikh\n",
            "Abdullah\n",
            "al\n",
            "Ani\n",
            "the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = len(tokenized)\n",
        "print('THe total number of words in the dataset')\n",
        "num_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRaGsqBdcGnt",
        "outputId": "1237bb5d-23c4-4918-bf8c-383f175a0051"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THe total number of words in the dataset\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "171448"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count word frequencies\n",
        "word_counts = Counter(tokenized)\n",
        "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "for word, count in sorted_word_counts[:10]:\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tsAn8xMgBLo",
        "outputId": "7e0fb702-1eec-4c66-904f-f99aaeb1ed89"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the: 8151\n",
            "to: 5077\n",
            "and: 4855\n",
            "of: 3589\n",
            "in: 2911\n",
            "is: 2178\n",
            "you: 2029\n",
            "that: 1981\n",
            "for: 1789\n",
            "it: 1588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To identify common verbs and nouns, two dictionary of nouns and verbs are made and the common words in both dictionaries will be counted.\n",
        "Language model of en_core_web_sm in soacy library is used as facilator.**"
      ],
      "metadata": {
        "id": "uxnuJjp1bVZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "chunks = text.split('.')\n",
        "noun_freq = {}\n",
        "verb_freq = {}\n",
        "\n",
        "for chunk in chunks:\n",
        "    doc = nlp(chunk)\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'NOUN':\n",
        "            noun_freq[token.text] = noun_freq.get(token.text, 0) + 1\n",
        "        elif token.pos_ == 'VERB':\n",
        "            verb_freq[token.text] = verb_freq.get(token.text, 0) + 1"
      ],
      "metadata": {
        "id": "haw2G-vyWuFJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of nouns:\", sum(noun_freq.values()))\n",
        "print(\"Total number of verbs:\", sum(verb_freq.values()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5pGJwCLd2Vx",
        "outputId": "c9c0d622-7f0c-4e10-bc1b-6f9a45f37904"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of nouns: 33908\n",
            "Total number of verbs: 23363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_noun_freq = sorted(noun_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"Top 10 NOUN:\")\n",
        "for noun, freq in sorted_noun_freq[:10]:\n",
        "    print(f\"{noun}: {freq}\")\n",
        "\n",
        "sorted_verb_freq = sorted(verb_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nTop 10 VERB:\")\n",
        "for verb, freq in sorted_verb_freq[:10]:\n",
        "    print(f\"{verb}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0GZAw1OYDrT",
        "outputId": "aef77101-be7b-4a52-cf23-b5cf43b4a872"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 NOUN:\n",
            "time: 387\n",
            "=: 312\n",
            "people: 234\n",
            "way: 193\n",
            "place: 181\n",
            "year: 174\n",
            "day: 168\n",
            "food: 158\n",
            "service: 157\n",
            "Thanks: 154\n",
            "\n",
            "Top 10 VERB:\n",
            "have: 736\n",
            "get: 355\n",
            "know: 338\n",
            "had: 318\n",
            "do: 273\n",
            "go: 260\n",
            "said: 233\n",
            "want: 207\n",
            "going: 191\n",
            "see: 186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "common_words = set(noun_freq.keys()).intersection(set(verb_freq.keys()))\n",
        "print(\"Number of common words between nouns and verbs:\", len(common_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAmPSXp7YxgR",
        "outputId": "228b6ba5-521b-48b9-9c5a-6d13df408ee3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of common words between nouns and verbs: 791\n"
          ]
        }
      ]
    }
  ]
}