{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fc40e427dd1e4ae2a4db7e33dba1aec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20031b5668e840349bfd42c9d075d845",
              "IPY_MODEL_050333c4b2a14ee0ad9282a65fabf28b",
              "IPY_MODEL_0049b0fa049047f9808e958398801135"
            ],
            "layout": "IPY_MODEL_9e6aab11ea3e4967b923651b3af38c90"
          }
        },
        "20031b5668e840349bfd42c9d075d845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af39388489084b94b46afd34eccf5e5c",
            "placeholder": "​",
            "style": "IPY_MODEL_fb7e6ddafe694a089a3bd8307915fd1e",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "050333c4b2a14ee0ad9282a65fabf28b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f80c37c2de04a9a8691873568b945dd",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74431080b2924dc5880bfcfc949f09d6",
            "value": 25000
          }
        },
        "0049b0fa049047f9808e958398801135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ece8004bc3343fb87753fc805a3085e",
            "placeholder": "​",
            "style": "IPY_MODEL_3f0b5bb4498f4ffa9aaefd5e6b496229",
            "value": " 25000/25000 [00:24&lt;00:00, 1119.47 examples/s]"
          }
        },
        "9e6aab11ea3e4967b923651b3af38c90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af39388489084b94b46afd34eccf5e5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb7e6ddafe694a089a3bd8307915fd1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f80c37c2de04a9a8691873568b945dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74431080b2924dc5880bfcfc949f09d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ece8004bc3343fb87753fc805a3085e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f0b5bb4498f4ffa9aaefd5e6b496229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6160292c3f6747bdadee2a9b23480e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d132801e47a4060ae647453e3c173b6",
              "IPY_MODEL_ebc206088c4c4861b4e41e3ffab337d4",
              "IPY_MODEL_2e145a75dc5d4f8e96a29451e5bef061"
            ],
            "layout": "IPY_MODEL_c4ba93cd09a845329e1d24b0dfc969e0"
          }
        },
        "1d132801e47a4060ae647453e3c173b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f34699ce4851465ba4a1b51e0762bd02",
            "placeholder": "​",
            "style": "IPY_MODEL_3788a0c0c18d408d9af5464e4c272818",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "ebc206088c4c4861b4e41e3ffab337d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97df58d601894471834eb8a9fd6fb267",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59fa140520ea4605b26d73ff419b1e08",
            "value": 25000
          }
        },
        "2e145a75dc5d4f8e96a29451e5bef061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a37a997375494f39b8efeaf17ec22e87",
            "placeholder": "​",
            "style": "IPY_MODEL_f8d2be19078b430a96202d0e90df068f",
            "value": " 25000/25000 [00:23&lt;00:00, 961.71 examples/s]"
          }
        },
        "c4ba93cd09a845329e1d24b0dfc969e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f34699ce4851465ba4a1b51e0762bd02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3788a0c0c18d408d9af5464e4c272818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97df58d601894471834eb8a9fd6fb267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59fa140520ea4605b26d73ff419b1e08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a37a997375494f39b8efeaf17ec22e87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8d2be19078b430a96202d0e90df068f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Introduction-to-Human-Language-Technology/blob/main/course_project_Reza2024_FULL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to HLT Project\n",
        "\n",
        "- Student(s) Name(s): Mohammadreza Akhtari\n",
        "- Date: June 2024\n",
        "- Chosen Corpus: imdb\n",
        "- Contributions (if group project): -\n",
        "\n",
        "### Corpus information\n",
        "\n",
        "- Description of the chosen corpus: Large Movie Review Dataset. This is a dataset for binary sentiment classification. A set of 25,000 highly polar movie reviews for training, and 25,000 for testing.\n",
        "- Paper(s) and other published materials related to the corpus: Maas, Andrew, et al. \"Learning word vectors for sentiment analysis.\" Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies. 2011.\n",
        "- State-of-the-art performance (best published results) on this corpus: Opinion Mining on Movie Reviews Based on Deep Learning Models https://doi.org/10.32604/jai.2023.045617"
      ],
      "metadata": {
        "id": "ucyWlC5gbOyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 1. Setup"
      ],
      "metadata": {
        "id": "D5d-9uxrcDY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to install and import libraries etc. here\n",
        "!pip3 install -q transformers[torch] datasets evaluate optuna plotly"
      ],
      "metadata": {
        "id": "caHHQoqEcG1J"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Data download and preprocessing\n",
        "\n",
        "### 2.1. Download the corpus"
      ],
      "metadata": {
        "id": "ovUapilSb8iT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PDx40YyzbGPc",
        "outputId": "ef310e72-bbb4-4a7a-e112-5ebdbeeb8c23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Your code to download the corpus here\n",
        "from pprint import pprint #pprint\n",
        "import datasets\n",
        "dset1=datasets.load_dataset(\"imdb\")\n",
        "pprint(dset1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dset2=dset1.shuffle() #This is never a bad idea, datasets may have ordering to them, which is not what we want\n",
        "del dset2[\"unsupervised\"] #Delete the unlabeled part of the dataset, we don't need it for anything"
      ],
      "metadata": {
        "id": "HrIjjs78eIxZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(dset2['train'][0]['text'])\n",
        "print(dset2['train'][0]['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B842NWr0ebA1",
        "outputId": "a0658730-9ea5-41c2-cb2d-8ccd0b2d18d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Okay, first of I hate commenting on this thing but I felt like I had to '\n",
            " 'stand up for this movie. So many people were bashing on it and I felt like '\n",
            " 'people who might want to see it should get a second opinion.<br /><br '\n",
            " '/>First off, Bend It Like Beckham is not meant to be the most profound movie '\n",
            " \"of the century. If that's what you're looking for go somewhere else. Just \"\n",
            " \"because it is an independent film does not mean it has to be artsy. It's \"\n",
            " \"supposed to make you feel good and you're supposed to have fun watching it \"\n",
            " 'and those two things are handily accomplished.<br /><br />Secondly, the '\n",
            " 'acting though not \"Halle Berry in Monster\\'s Ball\" is still good. The movie '\n",
            " \"doesn't need acting like that honestly so don't look for it. It's a family \"\n",
            " \"movie. If that's what you wanted you wouldn't or shouldn't even be looking \"\n",
            " 'into this movie honestly.<br /><br />Lastly, It has a really cute story. I '\n",
            " \"think it's thought out well and it's entertaining to watch. It's also very \"\n",
            " 'true to life for the most part for that culture so if you want to sit down '\n",
            " \"and watch a movie that you can enjoy and feel good about when you're \"\n",
            " \"finished. If you're looking for something with deep thought out plot lines \"\n",
            " 'and big dramatic scenes this is not for you.<br /><br />-Lyndsay')\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Preprocessing"
      ],
      "metadata": {
        "id": "cXb7CQNCbZOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dset=dset2\n",
        "'''\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+', '<URL>', text)\n",
        "\n",
        "    # Remove HTML tags (if any)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove non-alphanumeric characters except spaces\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Join tokens back into a single string\n",
        "    text = ' '.join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_dataset(dataset):\n",
        "    cleaned_dataset = []\n",
        "    for example in dataset:\n",
        "        cleaned_example = {}\n",
        "        cleaned_example['text'] = preprocess_text(example['text'])\n",
        "        cleaned_example['label'] = example['label']\n",
        "        cleaned_dataset.append(cleaned_example)\n",
        "    return cleaned_dataset\n",
        "\n",
        "# Apply to the dataset\n",
        "dset = clean_dataset(dset2['train'])\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "evF6zgm30P1i",
        "outputId": "014db7a9-9beb-416a-d4a1-c7c846f2b143"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport re\\nimport nltk\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\n\\nnltk.download('punkt')\\nnltk.download('stopwords')\\n\\ndef preprocess_text(text):\\n    # Convert to lowercase\\n    text = text.lower()\\n\\n    # Remove URLs\\n    text = re.sub(r'https?://\\\\S+', '<URL>', text)\\n\\n    # Remove HTML tags (if any)\\n    text = re.sub(r'<.*?>', '', text)\\n\\n    # Remove non-alphanumeric characters except spaces\\n    text = re.sub(r'[^a-zA-Z\\\\s]', '', text)\\n\\n    # Tokenize text\\n    tokens = word_tokenize(text)\\n\\n    # Remove stopwords\\n    stop_words = set(stopwords.words('english'))\\n    tokens = [word for word in tokens if word not in stop_words]\\n\\n    # Join tokens back into a single string\\n    text = ' '.join(tokens)\\n\\n    return text\\n\\ndef clean_dataset(dataset):\\n    cleaned_dataset = []\\n    for example in dataset:\\n        cleaned_example = {}\\n        cleaned_example['text'] = preprocess_text(example['text'])\\n        cleaned_example['label'] = example['label']\\n        cleaned_dataset.append(cleaned_example)\\n    return cleaned_dataset\\n\\n# Apply to the dataset\\ndset = clean_dataset(dset2['train'])\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize and map vocabulary\n",
        "import sklearn.feature_extraction\n",
        "\n",
        "vectorizer=sklearn.feature_extraction.text.CountVectorizer(binary=True,max_features=20000)\n",
        "\n",
        "texts=[ex[\"text\"] for ex in dset[\"train\"]] #get a list of all texts from the training data\n",
        "vectorizer.fit(texts) #\"Trains\" the vectorizer, i.e. builds its vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "NWA_wBkhfDh9",
        "outputId": "54868ac3-4ba9-488c-8743-7c3840e1e93b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(binary=True, max_features=20000)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True, max_features=20000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True, max_features=20000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the feature vectors\n",
        "def vectorize_example(ex):\n",
        "    vectorized=vectorizer.transform([ex[\"text\"]]) # [...] because the vectorizer expects a list/iterable over inputs, not one input\n",
        "    non_zero_features=vectorized.nonzero()[1] #.nonzero gives a pair of (rows,columns), we want the columns\n",
        "    non_zero_features+=1 #feature index 0 will have a special meaning\n",
        "                         # so let us not produce it by adding +1 to everything\n",
        "    return {\"input_ids\":non_zero_features}\n",
        "\n",
        "vectorized=vectorize_example(dset[\"train\"][0])"
      ],
      "metadata": {
        "id": "51o55guMfWU6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjwsunZifrjB",
        "outputId": "3ce3a22f-6b63-4575-c69e-6f7de17565b2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': array([  307,   369,   423,   775,   863,   891,  1117,  1205,  1555,\n",
            "        1660,  1703,  1738,  1741,  1834,  1872,  1927,  2300,  2612,\n",
            "        2717,  3002,  3637,  4431,  4484,  4730,  5400,  5401,  5436,\n",
            "        5500,  5545,  5951,  6102,  6142,  6307,  6645,  6770,  6787,\n",
            "        6884,  6925,  6947,  7139,  7385,  7618,  7743,  7783,  8117,\n",
            "        8148,  8278,  8292,  8311,  8669,  8921,  9084,  9158,  9485,\n",
            "        9595,  9621,  9875, 10222, 10427, 10453, 10491, 10620, 10624,\n",
            "       10874, 10958, 11169, 11178, 11365, 11612, 11701, 11746, 11980,\n",
            "       12188, 12348, 12349, 12391, 12421, 12463, 12492, 12567, 12846,\n",
            "       13023, 13351, 13819, 14304, 14343, 15489, 15650, 15652, 15678,\n",
            "       16023, 16026, 16199, 16469, 16541, 16546, 16887, 17019, 17076,\n",
            "       17413, 17887, 17891, 17945, 17946, 17948, 17961, 17973, 17975,\n",
            "       17976, 18112, 18462, 18549, 18919, 19112, 19352, 19353, 19412,\n",
            "       19418, 19507, 19519, 19538, 19548, 19593, 19711, 19805, 19926],\n",
            "      dtype=int32)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can map back to vocabulary and check that everything works\n",
        "# vectorizer.vocabulary_ is a dictionary {key:word, value:idx}\n",
        "\n",
        "idx2word=dict((i,w) for (w,i) in vectorizer.vocabulary_.items()) #inverse the vocab dictionary\n",
        "words=[]\n",
        "for idx in vectorized[\"input_ids\"]:\n",
        "    words.append(idx2word[idx-1]) ## It is easy to forgot we moved all by +1\n",
        "pprint(\", \".join(words)) #This is now the bag of words representation of the document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZH1pTbqhM_U",
        "outputId": "96b5a5ad-6fb0-4b25-fc1d-a2b73a1c4dec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('about, accomplished, acting, also, an, and, are, artsy, ball, bashing, be, '\n",
            " 'because, beckham, bend, berry, big, br, but, can, century, commenting, '\n",
            " 'culture, cute, deep, does, doesn, don, down, dramatic, else, enjoy, '\n",
            " 'entertaining, even, family, feel, felt, film, finished, first, for, fun, '\n",
            " 'get, go, good, had, halle, has, hate, have, honestly, if, in, independent, '\n",
            " 'into, is, it, just, lastly, life, like, lines, look, looking, make, many, '\n",
            " 'mean, meant, might, monster, most, movie, need, not, of, off, okay, on, '\n",
            " 'opinion, or, out, part, people, plot, profound, re, really, scenes, second, '\n",
            " 'secondly, see, should, shouldn, sit, so, something, somewhere, stand, still, '\n",
            " 'story, supposed, that, the, thing, things, think, this, those, though, '\n",
            " 'thought, to, true, two, up, very, want, wanted, watch, watching, well, were, '\n",
            " 'what, when, who, with, wouldn, you')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenizer to the whole dataset using .map()\n",
        "dset_tokenized = dset.map(vectorize_example,num_proc=4)\n",
        "pprint(dset_tokenized[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fc40e427dd1e4ae2a4db7e33dba1aec1",
            "20031b5668e840349bfd42c9d075d845",
            "050333c4b2a14ee0ad9282a65fabf28b",
            "0049b0fa049047f9808e958398801135",
            "9e6aab11ea3e4967b923651b3af38c90",
            "af39388489084b94b46afd34eccf5e5c",
            "fb7e6ddafe694a089a3bd8307915fd1e",
            "1f80c37c2de04a9a8691873568b945dd",
            "74431080b2924dc5880bfcfc949f09d6",
            "5ece8004bc3343fb87753fc805a3085e",
            "3f0b5bb4498f4ffa9aaefd5e6b496229",
            "6160292c3f6747bdadee2a9b23480e5b",
            "1d132801e47a4060ae647453e3c173b6",
            "ebc206088c4c4861b4e41e3ffab337d4",
            "2e145a75dc5d4f8e96a29451e5bef061",
            "c4ba93cd09a845329e1d24b0dfc969e0",
            "f34699ce4851465ba4a1b51e0762bd02",
            "3788a0c0c18d408d9af5464e4c272818",
            "97df58d601894471834eb8a9fd6fb267",
            "59fa140520ea4605b26d73ff419b1e08",
            "a37a997375494f39b8efeaf17ec22e87",
            "f8d2be19078b430a96202d0e90df068f"
          ]
        },
        "id": "G9kLWKfChb12",
        "outputId": "ae8fe8b7-2fb2-4e37-e4a3-c77882a4f1cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc40e427dd1e4ae2a4db7e33dba1aec1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6160292c3f6747bdadee2a9b23480e5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [307,\n",
            "               369,\n",
            "               423,\n",
            "               775,\n",
            "               863,\n",
            "               891,\n",
            "               1117,\n",
            "               1205,\n",
            "               1555,\n",
            "               1660,\n",
            "               1703,\n",
            "               1738,\n",
            "               1741,\n",
            "               1834,\n",
            "               1872,\n",
            "               1927,\n",
            "               2300,\n",
            "               2612,\n",
            "               2717,\n",
            "               3002,\n",
            "               3637,\n",
            "               4431,\n",
            "               4484,\n",
            "               4730,\n",
            "               5400,\n",
            "               5401,\n",
            "               5436,\n",
            "               5500,\n",
            "               5545,\n",
            "               5951,\n",
            "               6102,\n",
            "               6142,\n",
            "               6307,\n",
            "               6645,\n",
            "               6770,\n",
            "               6787,\n",
            "               6884,\n",
            "               6925,\n",
            "               6947,\n",
            "               7139,\n",
            "               7385,\n",
            "               7618,\n",
            "               7743,\n",
            "               7783,\n",
            "               8117,\n",
            "               8148,\n",
            "               8278,\n",
            "               8292,\n",
            "               8311,\n",
            "               8669,\n",
            "               8921,\n",
            "               9084,\n",
            "               9158,\n",
            "               9485,\n",
            "               9595,\n",
            "               9621,\n",
            "               9875,\n",
            "               10222,\n",
            "               10427,\n",
            "               10453,\n",
            "               10491,\n",
            "               10620,\n",
            "               10624,\n",
            "               10874,\n",
            "               10958,\n",
            "               11169,\n",
            "               11178,\n",
            "               11365,\n",
            "               11612,\n",
            "               11701,\n",
            "               11746,\n",
            "               11980,\n",
            "               12188,\n",
            "               12348,\n",
            "               12349,\n",
            "               12391,\n",
            "               12421,\n",
            "               12463,\n",
            "               12492,\n",
            "               12567,\n",
            "               12846,\n",
            "               13023,\n",
            "               13351,\n",
            "               13819,\n",
            "               14304,\n",
            "               14343,\n",
            "               15489,\n",
            "               15650,\n",
            "               15652,\n",
            "               15678,\n",
            "               16023,\n",
            "               16026,\n",
            "               16199,\n",
            "               16469,\n",
            "               16541,\n",
            "               16546,\n",
            "               16887,\n",
            "               17019,\n",
            "               17076,\n",
            "               17413,\n",
            "               17887,\n",
            "               17891,\n",
            "               17945,\n",
            "               17946,\n",
            "               17948,\n",
            "               17961,\n",
            "               17973,\n",
            "               17975,\n",
            "               17976,\n",
            "               18112,\n",
            "               18462,\n",
            "               18549,\n",
            "               18919,\n",
            "               19112,\n",
            "               19352,\n",
            "               19353,\n",
            "               19412,\n",
            "               19418,\n",
            "               19507,\n",
            "               19519,\n",
            "               19538,\n",
            "               19548,\n",
            "               19593,\n",
            "               19711,\n",
            "               19805,\n",
            "               19926],\n",
            " 'label': 1,\n",
            " 'text': 'Okay, first of I hate commenting on this thing but I felt like I had '\n",
            "         'to stand up for this movie. So many people were bashing on it and I '\n",
            "         'felt like people who might want to see it should get a second '\n",
            "         'opinion.<br /><br />First off, Bend It Like Beckham is not meant to '\n",
            "         \"be the most profound movie of the century. If that's what you're \"\n",
            "         'looking for go somewhere else. Just because it is an independent '\n",
            "         \"film does not mean it has to be artsy. It's supposed to make you \"\n",
            "         \"feel good and you're supposed to have fun watching it and those two \"\n",
            "         'things are handily accomplished.<br /><br />Secondly, the acting '\n",
            "         'though not \"Halle Berry in Monster\\'s Ball\" is still good. The movie '\n",
            "         \"doesn't need acting like that honestly so don't look for it. It's a \"\n",
            "         \"family movie. If that's what you wanted you wouldn't or shouldn't \"\n",
            "         'even be looking into this movie honestly.<br /><br />Lastly, It has '\n",
            "         \"a really cute story. I think it's thought out well and it's \"\n",
            "         \"entertaining to watch. It's also very true to life for the most part \"\n",
            "         'for that culture so if you want to sit down and watch a movie that '\n",
            "         \"you can enjoy and feel good about when you're finished. If you're \"\n",
            "         'looking for something with deep thought out plot lines and big '\n",
            "         'dramatic scenes this is not for you.<br /><br />-Lyndsay'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Padding and Collation (forming a batch)\n",
        "import torch\n",
        "\n",
        "def collator(list_of_examples):\n",
        "    batch={\"labels\":torch.tensor(list(ex[\"label\"] for ex in list_of_examples))} #this is easy, labels are made into a single tensor\n",
        "    #the worse bit is now to pad the examples, as they are of different length\n",
        "    tensors=[]\n",
        "    max_len=max(len(example[\"input_ids\"]) for example in list_of_examples) #this is the longest example in the batch\n",
        "    #everything needs to be padded to fit in length the longest example\n",
        "    #(so we can build a single tensor out of it)\n",
        "    for example in list_of_examples:\n",
        "        ids=torch.tensor(example[\"input_ids\"]) #pick the input ids\n",
        "        # pad(what,(from_left, from_right)) <- this is how we call the stock pad function\n",
        "        padded=torch.nn.functional.pad(ids,(0,max_len-ids.shape[0])) #pad by max - current length, pads with zero by default\n",
        "        tensors.append(padded) #accumulated the padded ids\n",
        "    batch[\"input_ids\"]=torch.vstack(tensors) #now that we have all of them the same length, a simple vstack() stacks them up\n",
        "    return batch #...and that's all there is to it"
      ],
      "metadata": {
        "id": "dvO_OQ6-hNC3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build a batch from 2 examples, with padding\n",
        "batch=collator([dset_tokenized[\"train\"][2],dset_tokenized[\"train\"][7]])\n",
        "print(\"Shape of labels:\",batch[\"labels\"].shape)\n",
        "print(\"Shape of input_ids:\",batch[\"input_ids\"].shape)\n",
        "pprint(batch[\"labels\"])\n",
        "pprint(batch[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZvwyummiyCW",
        "outputId": "52bb011a-ca18-4498-e8a9-1567dbf7e3df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of labels: torch.Size([2])\n",
            "Shape of input_ids: torch.Size([2, 442])\n",
            "tensor([0, 0])\n",
            "tensor([[   10,    17,   177,   250,   307,   318,   362,   423,   424,   434,\n",
            "           440,   472,   593,   652,   731,   765,   775,   787,   863,   891,\n",
            "           930,   963,  1011,  1013,  1048,  1117,  1209,  1214,  1291,  1302,\n",
            "          1318,  1326,  1337,  1411,  1433,  1494,  1516,  1603,  1654,  1661,\n",
            "          1703,  1738,  1757,  1900,  1996,  2045,  2101,  2148,  2166,  2300,\n",
            "          2633,  2684,  2697,  2748,  2916,  2935,  3002,  3076,  3162,  3317,\n",
            "          3679,  4054,  4127,  4461,  4637,  4694,  4695,  4778,  4912,  4934,\n",
            "          4947,  5093,  5165,  5169,  5171,  5255,  5377,  5400,  5409,  5436,\n",
            "          5500,  5714,  5728,  5753,  6127,  6142,  6249,  6307,  6316,  6324,\n",
            "          6589,  6668,  6770,  6884,  6911,  6960,  7132,  7166,  7344,  7403,\n",
            "          7468,  7618,  7620,  7621,  7671,  7783,  7998,  8078,  8278,  8311,\n",
            "          8339,  8340,  8342,  8474,  8543,  8582,  8730,  8760,  8770,  8773,\n",
            "          8889,  8921,  9084,  9118,  9366,  9474,  9485,  9553,  9595,  9605,\n",
            "          9610,  9621,  9758,  9875, 10121, 10376, 10421, 10427, 10453, 10486,\n",
            "         10536, 10620, 10671, 10839, 10874, 10878, 11098, 11123, 11258, 11405,\n",
            "         11443, 11657, 11674, 11701, 11711, 11746, 11749, 11763, 11844, 11946,\n",
            "         11964, 12042, 12121, 12188, 12197, 12348, 12382, 12394, 12421, 12423,\n",
            "         12429, 12492, 12554, 12567, 12725, 12781, 13095, 13287, 13351, 13668,\n",
            "         13801, 14123, 14224, 14246, 14290, 14343, 14444, 14625, 14629, 14640,\n",
            "         14678, 14898, 14999, 15261, 15328, 15473, 15486, 15489, 15590, 15678,\n",
            "         15692, 15745, 15815, 15982, 16001, 16020, 16022, 16023, 16130, 16355,\n",
            "         16376, 16394, 16469, 16533, 16537, 16541, 16542, 16546, 16559, 16597,\n",
            "         16602, 16851, 16952, 17097, 17304, 17311, 17313, 17350, 17413, 17499,\n",
            "         17620, 17723, 17772, 17818, 17824, 17836, 17879, 17887, 17891, 17901,\n",
            "         17910, 17923, 17938, 17945, 17946, 17948, 17961, 17973, 17976, 18008,\n",
            "         18014, 18073, 18112, 18163, 18251, 18304, 18348, 18535, 18614, 18752,\n",
            "         18919, 18956, 18986, 19112, 19144, 19355, 19396, 19412, 19438, 19519,\n",
            "         19538, 19548, 19549, 19593, 19639, 19711, 19769, 19772, 19795, 19803,\n",
            "         19847, 19848, 19897, 19913, 19926, 19932, 19990, 19991,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [   10,   164,   184,   237,   307,   413,   424,   434,   440,   554,\n",
            "           555,   593,   598,   654,   661,   731,   750,   762,   764,   794,\n",
            "           806,   863,   891,   914,  1008,  1117,  1156,  1159,  1171,  1209,\n",
            "          1233,  1239,  1247,  1302,  1332,  1433,  1461,  1494,  1574,  1703,\n",
            "          1757,  1778,  1780,  1795,  1927,  1944,  1981,  1988,  1996,  2046,\n",
            "          2087,  2110,  2204,  2228,  2253,  2300,  2422,  2542,  2550,  2558,\n",
            "          2612,  2633,  2682,  2706,  2717,  2736,  2778,  2788,  2853,  2935,\n",
            "          3056,  3076,  3146,  3222,  3318,  3319,  3339,  3388,  3518,  3601,\n",
            "          3618,  3638,  3643,  3655,  3946,  4138,  4139,  4156,  4168,  4176,\n",
            "          4234,  4457,  4492,  4548,  4694,  4811,  5060,  5093,  5161,  5162,\n",
            "          5359,  5377,  5412,  5436,  5597,  5625,  5663,  5951,  6029,  6051,\n",
            "          6142,  6241,  6274,  6315,  6324,  6350,  6589,  6630,  6649,  6701,\n",
            "          6752,  6773,  6788,  6793,  6863,  6884,  6894,  6902,  6914,  6947,\n",
            "          7111,  7139,  7144,  7224,  7293,  7313,  7344,  7345,  7385,  7403,\n",
            "          7436,  7562,  7564,  7607,  7610,  7618,  7635,  7674,  7682,  7743,\n",
            "          7747,  7783,  7816,  7910,  7920,  8024,  8093,  8136,  8179,  8191,\n",
            "          8212,  8278,  8311,  8312,  8339,  8410,  8426,  8432,  8451,  8474,\n",
            "          8476,  8483,  8557,  8571,  8582,  8630,  8647,  8669,  8759,  8809,\n",
            "          8861,  8942,  8988,  8991,  9084,  9113,  9335,  9366,  9386,  9448,\n",
            "          9485,  9595,  9621,  9624,  9629,  9768,  9776,  9875,  9881,  9998,\n",
            "         10068, 10075, 10224, 10241, 10285, 10286, 10376, 10384, 10453, 10536,\n",
            "         10547, 10561, 10671, 10683, 10685, 10727, 10817, 10823, 10837, 10874,\n",
            "         10878, 10881, 10906, 11026, 11063, 11123, 11125, 11164, 11178, 11213,\n",
            "         11365, 11382, 11393, 11445, 11448, 11495, 11533, 11657, 11739, 11765,\n",
            "         11779, 11810, 11844, 11884, 11930, 11987, 12042, 12047, 12056, 12086,\n",
            "         12121, 12154, 12188, 12197, 12222, 12348, 12349, 12370, 12394, 12421,\n",
            "         12423, 12429, 12458, 12492, 12564, 12567, 12618, 12668, 12712, 12730,\n",
            "         12802, 12879, 12958, 13023, 13036, 13186, 13193, 13317, 13343, 13418,\n",
            "         13645, 13787, 13922, 14087, 14141, 14168, 14224, 14317, 14321, 14334,\n",
            "         14335, 14343, 14423, 14434, 14444, 14686, 14749, 14818, 14975, 14994,\n",
            "         14999, 15203, 15204, 15224, 15261, 15278, 15315, 15320, 15328, 15445,\n",
            "         15446, 15486, 15489, 15655, 15673, 15678, 15682, 15705, 15779, 15805,\n",
            "         15988, 16004, 16008, 16013, 16018, 16023, 16046, 16048, 16093, 16130,\n",
            "         16154, 16163, 16469, 16533, 16537, 16541, 16545, 16613, 16619, 16645,\n",
            "         16763, 16821, 16887, 16903, 16959, 17046, 17197, 17259, 17364, 17413,\n",
            "         17517, 17609, 17620, 17625, 17634, 17660, 17763, 17826, 17836, 17879,\n",
            "         17880, 17885, 17887, 17889, 17891, 17901, 17904, 17910, 17911, 17923,\n",
            "         17932, 17938, 17945, 17948, 17961, 17973, 17975, 17990, 17992, 18010,\n",
            "         18028, 18066, 18107, 18112, 18123, 18132, 18172, 18208, 18215, 18232,\n",
            "         18234, 18282, 18404, 18423, 18519, 18523, 18536, 18549, 18642, 18695,\n",
            "         18919, 18959, 18961, 18978, 18981, 18984, 19049, 19055, 19329, 19357,\n",
            "         19380, 19396, 19406, 19412, 19438, 19444, 19507, 19538, 19549, 19550,\n",
            "         19558, 19593, 19636, 19639, 19652, 19673, 19695, 19711, 19714, 19743,\n",
            "         19769, 19803, 19831, 19848, 19857, 19897, 19902, 19910, 19913, 19926,\n",
            "         19932, 19934]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Machine learning model\n",
        "\n",
        "### 3.1. Model training"
      ],
      "metadata": {
        "id": "F1ntHh_JbrAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the MLP model\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "# A model wants a config, I can simply inherit from the base\n",
        "# class for pretrained configs\n",
        "class MLPConfig(transformers.PretrainedConfig):\n",
        "    pass\n",
        "\n",
        "# This is the model\n",
        "class MLP(transformers.PreTrainedModel):\n",
        "\n",
        "    config_class=MLPConfig\n",
        "\n",
        "    # In the initialization method, one instantiates the layers\n",
        "    # these will be, for the most part the trained parameters of the model\n",
        "    def __init__(self,config):\n",
        "        super().__init__(config)\n",
        "        self.vocab_size=config.vocab_size #embedding matrix row count\n",
        "        # Build and initialize embedding of vocab size +1 x hidden size (+1 because of the padding index 0!)\n",
        "        self.embedding=torch.nn.Embedding(num_embeddings=self.vocab_size+1,embedding_dim=config.hidden_size,padding_idx=0)\n",
        "        # Normally you would not initialize these yourself, but I have my reasons here ;)\n",
        "        torch.nn.init.uniform_(self.embedding.weight.data,-0.001,0.001) #initialize the embeddings with small random values\n",
        "        # Note! This is quite clever and keeps the embedding for 0, the padding, pure zeros\n",
        "        # This takes care of the lower half of the network, now the upper half\n",
        "        # Output layer: hidden size x output size\n",
        "        self.output=torch.nn.Linear(in_features=config.hidden_size,out_features=config.nlabels)\n",
        "        # Now we have the parameters of the model\n",
        "\n",
        "    def forward(self,input_ids,labels=None): #nevermind the attention_mask, its time will come, data collator insists on adding it\n",
        "        #sum up the embeddings of the items\n",
        "        embedded=self.embedding(input_ids) #(batch,ids)->(batch,ids,embedding_dim)\n",
        "        # Since the Embedding keeps the first row of the matrix pure zeros, we don't need to worry about the padding\n",
        "        # so next we sum the embeddings across the word dimension\n",
        "        # (batch,ids,embedding_dim) -> (batch,embedding_dim)\n",
        "        embedded_summed=torch.sum(embedded,dim=1)\n",
        "        logits=self.output(embedded_summed)\n",
        "\n",
        "        # We have labels, so we ought to calculate the loss\n",
        "        if labels is not None:\n",
        "            loss=torch.nn.CrossEntropyLoss() #This loss is meant for classification, so let's use it\n",
        "            # You run it as loss(model_output,correct_labels)\n",
        "            return (loss(logits,labels),logits)\n",
        "        else:\n",
        "            # No labels, so just return the logits\n",
        "            return (logits,)"
      ],
      "metadata": {
        "id": "2eK6rz0cJTSd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the model:\n",
        "mlp_config=MLPConfig(vocab_size=len(vectorizer.vocabulary_),hidden_size=1,nlabels=2)"
      ],
      "metadata": {
        "id": "DvHnlgv-nw-p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a model\n",
        "mlp=MLP(mlp_config)\n",
        "fake_batch=collator([dset_tokenized[\"train\"][0],dset_tokenized[\"train\"][1]])\n",
        "mlp(**fake_batch) #** expands input_ids and labels as parameters of the call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz2b7Eplnzxy",
        "outputId": "83fa993c-a390-4df0-f469-d9bb8818dad0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.2186, grad_fn=<NllLossBackward0>),\n",
              " tensor([[ 0.2972, -0.5967],\n",
              "         [ 0.2734, -0.5686]], grad_fn=<AddmmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training arguments\n",
        "# their names are mostly self-explanatory\n",
        "trainer_args = transformers.TrainingArguments(\n",
        "    \"mlp_checkpoints\", #save checkpoints here\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_steps=500,\n",
        "    learning_rate=1e-4, #learning rate of the gradient descent\n",
        "    max_steps=40000, #20000,\n",
        "    load_best_model_at_end=True,\n",
        "    per_device_train_batch_size=128\n",
        ")\n",
        "\n",
        "pprint(trainer_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkfGDmbwj0dL",
        "outputId": "de6d19e0-10cc-4417-9c27-2066764e0bcc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=500,\n",
            "eval_strategy=steps,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=mlp_checkpoints/runs/Jun16_13-24-37_77e5b2ccaa8a,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=40000,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=mlp_checkpoints,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=128,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=mlp_checkpoints,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A metric for evaluating performance\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_accuracy(outputs_and_labels):\n",
        "    outputs, labels = outputs_and_labels\n",
        "    predictions = np.argmax(outputs, axis=-1) #pick the index of the \"winning\" label\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "Gdbz-IoVoKib"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a new model\n",
        "mlp = MLP(mlp_config)\n",
        "\n",
        "\n",
        "# Argument gives the number of steps of patience before early stopping\n",
        "# i.e. training is stopped when the evaluation loss fails to improve\n",
        "# certain number of times\n",
        "early_stopping = transformers.EarlyStoppingCallback(5)\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=mlp,\n",
        "    args=trainer_args,\n",
        "    train_dataset=dset_tokenized[\"train\"],\n",
        "    eval_dataset=dset_tokenized[\"test\"].select(range(1000)), #make a smaller subset to evaluate on\n",
        "    compute_metrics=compute_accuracy,\n",
        "    data_collator=collator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# FINALLY!\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "id": "QA35ETsBkXtT",
        "outputId": "2b00eb86-c83f-4f57-a343-7f669ab197bb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9500' max='40000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 9500/40000 02:58 < 09:32, 53.30 it/s, Epoch 48/205]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.594100</td>\n",
              "      <td>0.512656</td>\n",
              "      <td>0.810000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.431700</td>\n",
              "      <td>0.420667</td>\n",
              "      <td>0.865000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.354500</td>\n",
              "      <td>0.370870</td>\n",
              "      <td>0.873000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.306500</td>\n",
              "      <td>0.338811</td>\n",
              "      <td>0.879000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.273200</td>\n",
              "      <td>0.317328</td>\n",
              "      <td>0.886000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.249200</td>\n",
              "      <td>0.302008</td>\n",
              "      <td>0.889000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.227100</td>\n",
              "      <td>0.290999</td>\n",
              "      <td>0.887000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.211000</td>\n",
              "      <td>0.282226</td>\n",
              "      <td>0.887000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.196900</td>\n",
              "      <td>0.276771</td>\n",
              "      <td>0.892000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.183800</td>\n",
              "      <td>0.271737</td>\n",
              "      <td>0.893000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.172700</td>\n",
              "      <td>0.268739</td>\n",
              "      <td>0.893000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.162700</td>\n",
              "      <td>0.267000</td>\n",
              "      <td>0.891000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.153000</td>\n",
              "      <td>0.266617</td>\n",
              "      <td>0.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.145400</td>\n",
              "      <td>0.265244</td>\n",
              "      <td>0.886000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.137800</td>\n",
              "      <td>0.265692</td>\n",
              "      <td>0.885000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.130900</td>\n",
              "      <td>0.266477</td>\n",
              "      <td>0.887000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.124000</td>\n",
              "      <td>0.267139</td>\n",
              "      <td>0.889000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.118100</td>\n",
              "      <td>0.268626</td>\n",
              "      <td>0.891000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.113200</td>\n",
              "      <td>0.269809</td>\n",
              "      <td>0.891000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9500, training_loss=0.22557449140046773, metrics={'train_runtime': 179.0629, 'train_samples_per_second': 28593.299, 'train_steps_per_second': 223.385, 'total_flos': 12420685248.0, 'train_loss': 0.22557449140046773, 'epoch': 48.46938775510204})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation\n",
        "eval_results = trainer.evaluate(dset_tokenized[\"test\"])\n",
        "\n",
        "print(eval_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "_44sUxcikY6Y",
        "outputId": "0a441119-2149-4660-a1a5-88d77648ba82"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3125/3125 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2811674475669861, 'eval_accuracy': 0.88688, 'eval_runtime': 6.4718, 'eval_samples_per_second': 3862.918, 'eval_steps_per_second': 482.865, 'epoch': 48.46938775510204}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Hyperparameter optimization"
      ],
      "metadata": {
        "id": "nlO8RVuHcmAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Evaluation on test set"
      ],
      "metadata": {
        "id": "1EzCYTnfcrvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # Define the search space for hyperparameters\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 64, 128, 256])\n",
        "\n",
        "    trainer_args = transformers.TrainingArguments(\n",
        "        \"mlp_checkpoints\", #save checkpoints here\n",
        "        evaluation_strategy=\"steps\",\n",
        "        logging_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        logging_steps=500,\n",
        "        learning_rate=learning_rate, #learning rate of the gradient descent\n",
        "        max_steps=40000, #10000, #I will keep this small, no time to wait too long :)\n",
        "        load_best_model_at_end=True,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    mlp = MLP(mlp_config)\n",
        "\n",
        "    trainer = transformers.Trainer(\n",
        "        model=mlp,\n",
        "        args=trainer_args,\n",
        "        train_dataset=dset_tokenized[\"train\"],\n",
        "        eval_dataset=dset_tokenized[\"test\"], #dset_tokenized[\"test\"].select(range(1000)), #make a smaller subset to evaluate on\n",
        "        compute_metrics=compute_accuracy,\n",
        "        data_collator=collator,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Train the model and get the best validation loss\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate(eval_dataset=dset_tokenized[\"test\"]) #trainer.evaluate()\n",
        "    return eval_results[\"eval_accuracy\"] #let's try to maximize accuracy\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=7) #I will keep this small, no time to wait :)"
      ],
      "metadata": {
        "id": "IzDrTDd0cWOG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "004988af-421d-495c-b140-2799f059f681"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-16 13:27:47,915] A new study created in memory with name: no-name-c6113b77-9620-40ae-891c-c9d03369f71f\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4000' max='40000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 4000/40000 01:00 < 09:04, 66.15 it/s, Epoch 2/26]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.426900</td>\n",
              "      <td>0.311226</td>\n",
              "      <td>0.873000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.283600</td>\n",
              "      <td>0.305410</td>\n",
              "      <td>0.874520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.284700</td>\n",
              "      <td>0.292088</td>\n",
              "      <td>0.882840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.158600</td>\n",
              "      <td>0.324944</td>\n",
              "      <td>0.878640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.167100</td>\n",
              "      <td>0.338984</td>\n",
              "      <td>0.874960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.348777</td>\n",
              "      <td>0.871280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.107800</td>\n",
              "      <td>0.425744</td>\n",
              "      <td>0.864480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.093500</td>\n",
              "      <td>0.460432</td>\n",
              "      <td>0.863840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1563/1563 00:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-16 13:28:53,214] Trial 0 finished with value: 0.88284 and parameters: {'learning_rate': 0.0028405788477162846, 'batch_size': 16}. Best is trial 0 with value: 0.88284.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40000' max='40000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40000/40000 11:41, Epoch 102/103]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.736900</td>\n",
              "      <td>0.708646</td>\n",
              "      <td>0.508840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.686900</td>\n",
              "      <td>0.677928</td>\n",
              "      <td>0.551680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.660500</td>\n",
              "      <td>0.653874</td>\n",
              "      <td>0.606040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.632300</td>\n",
              "      <td>0.627884</td>\n",
              "      <td>0.673240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.601300</td>\n",
              "      <td>0.599800</td>\n",
              "      <td>0.736600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.572500</td>\n",
              "      <td>0.571816</td>\n",
              "      <td>0.778400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.541000</td>\n",
              "      <td>0.544707</td>\n",
              "      <td>0.807240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.511800</td>\n",
              "      <td>0.519361</td>\n",
              "      <td>0.826800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.486600</td>\n",
              "      <td>0.496391</td>\n",
              "      <td>0.838280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.460100</td>\n",
              "      <td>0.475587</td>\n",
              "      <td>0.845960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.439700</td>\n",
              "      <td>0.456882</td>\n",
              "      <td>0.851880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.419900</td>\n",
              "      <td>0.440260</td>\n",
              "      <td>0.855880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.400100</td>\n",
              "      <td>0.424963</td>\n",
              "      <td>0.859880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.386200</td>\n",
              "      <td>0.411481</td>\n",
              "      <td>0.863240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.371900</td>\n",
              "      <td>0.399315</td>\n",
              "      <td>0.866040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.354700</td>\n",
              "      <td>0.388508</td>\n",
              "      <td>0.868160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.378183</td>\n",
              "      <td>0.870480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.334700</td>\n",
              "      <td>0.369270</td>\n",
              "      <td>0.871960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.324000</td>\n",
              "      <td>0.361247</td>\n",
              "      <td>0.873880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.313700</td>\n",
              "      <td>0.353880</td>\n",
              "      <td>0.874640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.304100</td>\n",
              "      <td>0.346970</td>\n",
              "      <td>0.876240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.297300</td>\n",
              "      <td>0.340835</td>\n",
              "      <td>0.876960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.288900</td>\n",
              "      <td>0.335237</td>\n",
              "      <td>0.877600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.330118</td>\n",
              "      <td>0.878440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.275100</td>\n",
              "      <td>0.325581</td>\n",
              "      <td>0.878880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.269400</td>\n",
              "      <td>0.321122</td>\n",
              "      <td>0.880280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.263500</td>\n",
              "      <td>0.317429</td>\n",
              "      <td>0.880640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.258600</td>\n",
              "      <td>0.313736</td>\n",
              "      <td>0.881640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.253600</td>\n",
              "      <td>0.310640</td>\n",
              "      <td>0.882280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.248700</td>\n",
              "      <td>0.307334</td>\n",
              "      <td>0.882640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.243500</td>\n",
              "      <td>0.304800</td>\n",
              "      <td>0.883520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.240600</td>\n",
              "      <td>0.302279</td>\n",
              "      <td>0.883960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.234800</td>\n",
              "      <td>0.299940</td>\n",
              "      <td>0.883880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.232200</td>\n",
              "      <td>0.297828</td>\n",
              "      <td>0.884720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.227500</td>\n",
              "      <td>0.295969</td>\n",
              "      <td>0.885120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.225000</td>\n",
              "      <td>0.294187</td>\n",
              "      <td>0.885400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.222800</td>\n",
              "      <td>0.292532</td>\n",
              "      <td>0.885800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.217500</td>\n",
              "      <td>0.291104</td>\n",
              "      <td>0.886640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.215100</td>\n",
              "      <td>0.289822</td>\n",
              "      <td>0.886760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.213500</td>\n",
              "      <td>0.288543</td>\n",
              "      <td>0.887160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.209300</td>\n",
              "      <td>0.287463</td>\n",
              "      <td>0.887240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.207200</td>\n",
              "      <td>0.286426</td>\n",
              "      <td>0.887640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.205700</td>\n",
              "      <td>0.285370</td>\n",
              "      <td>0.887720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.203300</td>\n",
              "      <td>0.284402</td>\n",
              "      <td>0.887680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.201000</td>\n",
              "      <td>0.283687</td>\n",
              "      <td>0.888160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.198500</td>\n",
              "      <td>0.283130</td>\n",
              "      <td>0.887960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.195200</td>\n",
              "      <td>0.282237</td>\n",
              "      <td>0.888240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.193500</td>\n",
              "      <td>0.281690</td>\n",
              "      <td>0.888720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.195600</td>\n",
              "      <td>0.281157</td>\n",
              "      <td>0.888600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.190000</td>\n",
              "      <td>0.280722</td>\n",
              "      <td>0.888640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.189500</td>\n",
              "      <td>0.280252</td>\n",
              "      <td>0.888920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.186500</td>\n",
              "      <td>0.280007</td>\n",
              "      <td>0.888760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.186600</td>\n",
              "      <td>0.279463</td>\n",
              "      <td>0.889200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.185900</td>\n",
              "      <td>0.279189</td>\n",
              "      <td>0.889000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.181800</td>\n",
              "      <td>0.278867</td>\n",
              "      <td>0.889080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.182200</td>\n",
              "      <td>0.278527</td>\n",
              "      <td>0.888880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.181100</td>\n",
              "      <td>0.278318</td>\n",
              "      <td>0.888920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.179100</td>\n",
              "      <td>0.278137</td>\n",
              "      <td>0.888920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.181200</td>\n",
              "      <td>0.277846</td>\n",
              "      <td>0.889320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.175600</td>\n",
              "      <td>0.277744</td>\n",
              "      <td>0.889200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.177400</td>\n",
              "      <td>0.277608</td>\n",
              "      <td>0.889280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.176200</td>\n",
              "      <td>0.277472</td>\n",
              "      <td>0.889400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>0.174800</td>\n",
              "      <td>0.277247</td>\n",
              "      <td>0.889560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.173600</td>\n",
              "      <td>0.277241</td>\n",
              "      <td>0.889520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>0.172600</td>\n",
              "      <td>0.277120</td>\n",
              "      <td>0.889640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.172800</td>\n",
              "      <td>0.277054</td>\n",
              "      <td>0.889640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>0.170700</td>\n",
              "      <td>0.276900</td>\n",
              "      <td>0.889880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>0.171000</td>\n",
              "      <td>0.276866</td>\n",
              "      <td>0.889960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>0.172000</td>\n",
              "      <td>0.276836</td>\n",
              "      <td>0.889840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>0.169400</td>\n",
              "      <td>0.276872</td>\n",
              "      <td>0.889560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>0.168300</td>\n",
              "      <td>0.276748</td>\n",
              "      <td>0.889760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>0.170100</td>\n",
              "      <td>0.276675</td>\n",
              "      <td>0.889840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>0.169300</td>\n",
              "      <td>0.276659</td>\n",
              "      <td>0.889840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>0.167700</td>\n",
              "      <td>0.276627</td>\n",
              "      <td>0.889880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>0.169100</td>\n",
              "      <td>0.276601</td>\n",
              "      <td>0.889920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>0.169100</td>\n",
              "      <td>0.276579</td>\n",
              "      <td>0.889880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38500</td>\n",
              "      <td>0.166100</td>\n",
              "      <td>0.276590</td>\n",
              "      <td>0.889960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>0.165900</td>\n",
              "      <td>0.276553</td>\n",
              "      <td>0.889840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39500</td>\n",
              "      <td>0.168700</td>\n",
              "      <td>0.276559</td>\n",
              "      <td>0.889920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>0.166000</td>\n",
              "      <td>0.276560</td>\n",
              "      <td>0.889920</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [391/391 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-16 13:40:38,683] Trial 1 finished with value: 0.88984 and parameters: {'learning_rate': 4.671898970848419e-05, 'batch_size': 64}. Best is trial 1 with value: 0.88984.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40000' max='40000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40000/40000 25:37, Epoch 408/409]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.811600</td>\n",
              "      <td>0.799005</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.784700</td>\n",
              "      <td>0.774826</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.762900</td>\n",
              "      <td>0.754881</td>\n",
              "      <td>0.499840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.744200</td>\n",
              "      <td>0.738565</td>\n",
              "      <td>0.501160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.729200</td>\n",
              "      <td>0.725289</td>\n",
              "      <td>0.507400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.717000</td>\n",
              "      <td>0.714312</td>\n",
              "      <td>0.510920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.706700</td>\n",
              "      <td>0.705166</td>\n",
              "      <td>0.514560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.698300</td>\n",
              "      <td>0.697423</td>\n",
              "      <td>0.524000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.690500</td>\n",
              "      <td>0.690765</td>\n",
              "      <td>0.530600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.684300</td>\n",
              "      <td>0.684846</td>\n",
              "      <td>0.538960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.678000</td>\n",
              "      <td>0.679517</td>\n",
              "      <td>0.549160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.672700</td>\n",
              "      <td>0.674614</td>\n",
              "      <td>0.557680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.667400</td>\n",
              "      <td>0.670007</td>\n",
              "      <td>0.565840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.662800</td>\n",
              "      <td>0.665649</td>\n",
              "      <td>0.575320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.657800</td>\n",
              "      <td>0.661491</td>\n",
              "      <td>0.584400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.653400</td>\n",
              "      <td>0.657503</td>\n",
              "      <td>0.593600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.649100</td>\n",
              "      <td>0.653647</td>\n",
              "      <td>0.602560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.644700</td>\n",
              "      <td>0.649922</td>\n",
              "      <td>0.610840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.640700</td>\n",
              "      <td>0.646289</td>\n",
              "      <td>0.621320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.636500</td>\n",
              "      <td>0.642757</td>\n",
              "      <td>0.629680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.632700</td>\n",
              "      <td>0.639330</td>\n",
              "      <td>0.638520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.628600</td>\n",
              "      <td>0.635997</td>\n",
              "      <td>0.646160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.625300</td>\n",
              "      <td>0.632747</td>\n",
              "      <td>0.653560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.621300</td>\n",
              "      <td>0.629588</td>\n",
              "      <td>0.662160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.618200</td>\n",
              "      <td>0.626512</td>\n",
              "      <td>0.670960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.614500</td>\n",
              "      <td>0.623524</td>\n",
              "      <td>0.677920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.611000</td>\n",
              "      <td>0.620617</td>\n",
              "      <td>0.684960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.608000</td>\n",
              "      <td>0.617790</td>\n",
              "      <td>0.692760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.604800</td>\n",
              "      <td>0.615040</td>\n",
              "      <td>0.699800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.601600</td>\n",
              "      <td>0.612373</td>\n",
              "      <td>0.705680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.598600</td>\n",
              "      <td>0.609777</td>\n",
              "      <td>0.711000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.595700</td>\n",
              "      <td>0.607250</td>\n",
              "      <td>0.716280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.592900</td>\n",
              "      <td>0.604808</td>\n",
              "      <td>0.721720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.590300</td>\n",
              "      <td>0.602424</td>\n",
              "      <td>0.725800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.587200</td>\n",
              "      <td>0.600116</td>\n",
              "      <td>0.730360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.585300</td>\n",
              "      <td>0.597879</td>\n",
              "      <td>0.734400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.582200</td>\n",
              "      <td>0.595706</td>\n",
              "      <td>0.737760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.580100</td>\n",
              "      <td>0.593607</td>\n",
              "      <td>0.741520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.577600</td>\n",
              "      <td>0.591570</td>\n",
              "      <td>0.745200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.575400</td>\n",
              "      <td>0.589597</td>\n",
              "      <td>0.748440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.573000</td>\n",
              "      <td>0.587685</td>\n",
              "      <td>0.751480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.571400</td>\n",
              "      <td>0.585837</td>\n",
              "      <td>0.754240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.568700</td>\n",
              "      <td>0.584044</td>\n",
              "      <td>0.757240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.566900</td>\n",
              "      <td>0.582313</td>\n",
              "      <td>0.759880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.564800</td>\n",
              "      <td>0.580638</td>\n",
              "      <td>0.762360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.563100</td>\n",
              "      <td>0.579021</td>\n",
              "      <td>0.763840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.561300</td>\n",
              "      <td>0.577461</td>\n",
              "      <td>0.765480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.559600</td>\n",
              "      <td>0.575951</td>\n",
              "      <td>0.767160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.557800</td>\n",
              "      <td>0.574498</td>\n",
              "      <td>0.768560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.556200</td>\n",
              "      <td>0.573100</td>\n",
              "      <td>0.769960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.554500</td>\n",
              "      <td>0.571756</td>\n",
              "      <td>0.771680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.553200</td>\n",
              "      <td>0.570466</td>\n",
              "      <td>0.772640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.551800</td>\n",
              "      <td>0.569227</td>\n",
              "      <td>0.773840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.550300</td>\n",
              "      <td>0.568039</td>\n",
              "      <td>0.775320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.549100</td>\n",
              "      <td>0.566902</td>\n",
              "      <td>0.777440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.547700</td>\n",
              "      <td>0.565810</td>\n",
              "      <td>0.778800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.546200</td>\n",
              "      <td>0.564769</td>\n",
              "      <td>0.780320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.545400</td>\n",
              "      <td>0.563776</td>\n",
              "      <td>0.781240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.544200</td>\n",
              "      <td>0.562828</td>\n",
              "      <td>0.782600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.543200</td>\n",
              "      <td>0.561930</td>\n",
              "      <td>0.783800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.542200</td>\n",
              "      <td>0.561078</td>\n",
              "      <td>0.784400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.541400</td>\n",
              "      <td>0.560274</td>\n",
              "      <td>0.785040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>0.540300</td>\n",
              "      <td>0.559516</td>\n",
              "      <td>0.785440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.539500</td>\n",
              "      <td>0.558802</td>\n",
              "      <td>0.785920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>0.538600</td>\n",
              "      <td>0.558134</td>\n",
              "      <td>0.786720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.538000</td>\n",
              "      <td>0.557509</td>\n",
              "      <td>0.786680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>0.537200</td>\n",
              "      <td>0.556929</td>\n",
              "      <td>0.787080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>0.536800</td>\n",
              "      <td>0.556394</td>\n",
              "      <td>0.787600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>0.536100</td>\n",
              "      <td>0.555903</td>\n",
              "      <td>0.787920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>0.535700</td>\n",
              "      <td>0.555455</td>\n",
              "      <td>0.788080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>0.534900</td>\n",
              "      <td>0.555050</td>\n",
              "      <td>0.788440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>0.534500</td>\n",
              "      <td>0.554689</td>\n",
              "      <td>0.788800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>0.534200</td>\n",
              "      <td>0.554371</td>\n",
              "      <td>0.789120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>0.533800</td>\n",
              "      <td>0.554095</td>\n",
              "      <td>0.789360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>0.533600</td>\n",
              "      <td>0.553862</td>\n",
              "      <td>0.789800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>0.533300</td>\n",
              "      <td>0.553671</td>\n",
              "      <td>0.790120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38500</td>\n",
              "      <td>0.533200</td>\n",
              "      <td>0.553523</td>\n",
              "      <td>0.790080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>0.532900</td>\n",
              "      <td>0.553417</td>\n",
              "      <td>0.790120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39500</td>\n",
              "      <td>0.532900</td>\n",
              "      <td>0.553354</td>\n",
              "      <td>0.790120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>0.533000</td>\n",
              "      <td>0.553333</td>\n",
              "      <td>0.790160</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='98' max='98' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [98/98 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-16 14:06:19,234] Trial 2 finished with value: 0.79016 and parameters: {'learning_rate': 3.400610504968987e-06, 'batch_size': 256}. Best is trial 1 with value: 0.88984.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40000' max='40000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40000/40000 16:05, Epoch 204/205]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.769400</td>\n",
              "      <td>0.762173</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.754200</td>\n",
              "      <td>0.748564</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.740500</td>\n",
              "      <td>0.736414</td>\n",
              "      <td>0.500200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.728200</td>\n",
              "      <td>0.725974</td>\n",
              "      <td>0.503840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.718100</td>\n",
              "      <td>0.717085</td>\n",
              "      <td>0.507400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.708900</td>\n",
              "      <td>0.709734</td>\n",
              "      <td>0.513720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.702300</td>\n",
              "      <td>0.703415</td>\n",
              "      <td>0.520080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.695700</td>\n",
              "      <td>0.697971</td>\n",
              "      <td>0.524760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.690100</td>\n",
              "      <td>0.693141</td>\n",
              "      <td>0.529920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.685100</td>\n",
              "      <td>0.688753</td>\n",
              "      <td>0.535200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.680700</td>\n",
              "      <td>0.684650</td>\n",
              "      <td>0.541040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.675700</td>\n",
              "      <td>0.680743</td>\n",
              "      <td>0.545720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.671600</td>\n",
              "      <td>0.676932</td>\n",
              "      <td>0.551160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.668100</td>\n",
              "      <td>0.673200</td>\n",
              "      <td>0.558720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.663400</td>\n",
              "      <td>0.669511</td>\n",
              "      <td>0.565600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.659400</td>\n",
              "      <td>0.665864</td>\n",
              "      <td>0.573120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.655500</td>\n",
              "      <td>0.662227</td>\n",
              "      <td>0.579920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.651900</td>\n",
              "      <td>0.658615</td>\n",
              "      <td>0.587600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.647200</td>\n",
              "      <td>0.655037</td>\n",
              "      <td>0.596360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.643400</td>\n",
              "      <td>0.651459</td>\n",
              "      <td>0.603760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.639400</td>\n",
              "      <td>0.647913</td>\n",
              "      <td>0.612200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.635800</td>\n",
              "      <td>0.644413</td>\n",
              "      <td>0.619880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.632000</td>\n",
              "      <td>0.640941</td>\n",
              "      <td>0.627640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.627600</td>\n",
              "      <td>0.637489</td>\n",
              "      <td>0.635480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.624400</td>\n",
              "      <td>0.634089</td>\n",
              "      <td>0.642960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.620400</td>\n",
              "      <td>0.630719</td>\n",
              "      <td>0.650880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.616700</td>\n",
              "      <td>0.627405</td>\n",
              "      <td>0.657280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.612700</td>\n",
              "      <td>0.624125</td>\n",
              "      <td>0.665160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.609700</td>\n",
              "      <td>0.620896</td>\n",
              "      <td>0.671600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.605700</td>\n",
              "      <td>0.617712</td>\n",
              "      <td>0.679000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.602600</td>\n",
              "      <td>0.614600</td>\n",
              "      <td>0.685840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.599000</td>\n",
              "      <td>0.611534</td>\n",
              "      <td>0.692800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.595700</td>\n",
              "      <td>0.608544</td>\n",
              "      <td>0.698600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.592100</td>\n",
              "      <td>0.605609</td>\n",
              "      <td>0.704480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.589500</td>\n",
              "      <td>0.602736</td>\n",
              "      <td>0.710440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.586100</td>\n",
              "      <td>0.599933</td>\n",
              "      <td>0.716120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.582900</td>\n",
              "      <td>0.597195</td>\n",
              "      <td>0.720120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.579800</td>\n",
              "      <td>0.594510</td>\n",
              "      <td>0.724600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.576600</td>\n",
              "      <td>0.591888</td>\n",
              "      <td>0.729040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.574500</td>\n",
              "      <td>0.589333</td>\n",
              "      <td>0.733360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.571300</td>\n",
              "      <td>0.586845</td>\n",
              "      <td>0.737480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.569000</td>\n",
              "      <td>0.584437</td>\n",
              "      <td>0.740440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.565800</td>\n",
              "      <td>0.582079</td>\n",
              "      <td>0.743800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.563700</td>\n",
              "      <td>0.579801</td>\n",
              "      <td>0.747560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.560600</td>\n",
              "      <td>0.577585</td>\n",
              "      <td>0.751200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.558500</td>\n",
              "      <td>0.575438</td>\n",
              "      <td>0.754560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.556500</td>\n",
              "      <td>0.573348</td>\n",
              "      <td>0.758160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.553800</td>\n",
              "      <td>0.571323</td>\n",
              "      <td>0.760520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.551600</td>\n",
              "      <td>0.569368</td>\n",
              "      <td>0.763080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.549700</td>\n",
              "      <td>0.567484</td>\n",
              "      <td>0.764280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.547600</td>\n",
              "      <td>0.565656</td>\n",
              "      <td>0.765960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.545500</td>\n",
              "      <td>0.563903</td>\n",
              "      <td>0.768160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.543500</td>\n",
              "      <td>0.562209</td>\n",
              "      <td>0.770640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.541900</td>\n",
              "      <td>0.560584</td>\n",
              "      <td>0.772480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.539800</td>\n",
              "      <td>0.559021</td>\n",
              "      <td>0.774800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.538500</td>\n",
              "      <td>0.557525</td>\n",
              "      <td>0.776560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.536800</td>\n",
              "      <td>0.556093</td>\n",
              "      <td>0.778320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.534800</td>\n",
              "      <td>0.554721</td>\n",
              "      <td>0.780040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.533900</td>\n",
              "      <td>0.553417</td>\n",
              "      <td>0.781880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.532300</td>\n",
              "      <td>0.552173</td>\n",
              "      <td>0.783640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.530800</td>\n",
              "      <td>0.550990</td>\n",
              "      <td>0.785160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.529700</td>\n",
              "      <td>0.549874</td>\n",
              "      <td>0.786320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>0.528400</td>\n",
              "      <td>0.548823</td>\n",
              "      <td>0.787480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.527000</td>\n",
              "      <td>0.547831</td>\n",
              "      <td>0.788800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>0.526600</td>\n",
              "      <td>0.546902</td>\n",
              "      <td>0.789960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.525600</td>\n",
              "      <td>0.546037</td>\n",
              "      <td>0.790520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>0.523900</td>\n",
              "      <td>0.545231</td>\n",
              "      <td>0.791320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>0.523300</td>\n",
              "      <td>0.544487</td>\n",
              "      <td>0.791880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>0.522800</td>\n",
              "      <td>0.543799</td>\n",
              "      <td>0.792320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>0.521900</td>\n",
              "      <td>0.543174</td>\n",
              "      <td>0.792880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>0.521300</td>\n",
              "      <td>0.542611</td>\n",
              "      <td>0.793440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>0.520700</td>\n",
              "      <td>0.542107</td>\n",
              "      <td>0.794000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>0.519900</td>\n",
              "      <td>0.541662</td>\n",
              "      <td>0.794360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>0.519800</td>\n",
              "      <td>0.541275</td>\n",
              "      <td>0.794840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>0.519500</td>\n",
              "      <td>0.540950</td>\n",
              "      <td>0.795120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>0.519300</td>\n",
              "      <td>0.540685</td>\n",
              "      <td>0.795400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38500</td>\n",
              "      <td>0.518800</td>\n",
              "      <td>0.540478</td>\n",
              "      <td>0.795440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>0.518200</td>\n",
              "      <td>0.540329</td>\n",
              "      <td>0.795640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39500</td>\n",
              "      <td>0.518300</td>\n",
              "      <td>0.540241</td>\n",
              "      <td>0.795760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>0.518400</td>\n",
              "      <td>0.540212</td>\n",
              "      <td>0.795760</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='196' max='196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [196/196 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-16 14:22:28,045] Trial 3 finished with value: 0.79576 and parameters: {'learning_rate': 7.079172599523785e-06, 'batch_size': 128}. Best is trial 1 with value: 0.88984.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3000' max='40000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3000/40000 00:52 < 10:42, 57.56 it/s, Epoch 7/103]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.277600</td>\n",
              "      <td>0.332116</td>\n",
              "      <td>0.875760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.128100</td>\n",
              "      <td>0.470496</td>\n",
              "      <td>0.856880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.077700</td>\n",
              "      <td>0.616265</td>\n",
              "      <td>0.851720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.047400</td>\n",
              "      <td>0.793780</td>\n",
              "      <td>0.845640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.031900</td>\n",
              "      <td>0.957075</td>\n",
              "      <td>0.847560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.029600</td>\n",
              "      <td>1.107733</td>\n",
              "      <td>0.842640</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [391/391 00:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-16 14:23:24,713] Trial 4 finished with value: 0.87576 and parameters: {'learning_rate': 0.006035009622352109, 'batch_size': 64}. Best is trial 1 with value: 0.88984.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40000' max='40000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40000/40000 25:48, Epoch 408/409]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.696100</td>\n",
              "      <td>0.693874</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.690800</td>\n",
              "      <td>0.688223</td>\n",
              "      <td>0.512160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.684200</td>\n",
              "      <td>0.681820</td>\n",
              "      <td>0.569040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.676800</td>\n",
              "      <td>0.674966</td>\n",
              "      <td>0.644560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.669000</td>\n",
              "      <td>0.667722</td>\n",
              "      <td>0.708440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.660500</td>\n",
              "      <td>0.660001</td>\n",
              "      <td>0.744600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.651300</td>\n",
              "      <td>0.651857</td>\n",
              "      <td>0.766160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.642000</td>\n",
              "      <td>0.643396</td>\n",
              "      <td>0.781040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.632100</td>\n",
              "      <td>0.634715</td>\n",
              "      <td>0.790600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.622200</td>\n",
              "      <td>0.625906</td>\n",
              "      <td>0.797320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.612100</td>\n",
              "      <td>0.617045</td>\n",
              "      <td>0.804720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.602100</td>\n",
              "      <td>0.608201</td>\n",
              "      <td>0.809120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.591900</td>\n",
              "      <td>0.599422</td>\n",
              "      <td>0.812600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.582000</td>\n",
              "      <td>0.590752</td>\n",
              "      <td>0.815440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.572300</td>\n",
              "      <td>0.582208</td>\n",
              "      <td>0.817920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.562600</td>\n",
              "      <td>0.573828</td>\n",
              "      <td>0.819840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.553100</td>\n",
              "      <td>0.565652</td>\n",
              "      <td>0.821560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.544100</td>\n",
              "      <td>0.557675</td>\n",
              "      <td>0.823480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.535000</td>\n",
              "      <td>0.549904</td>\n",
              "      <td>0.825480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.526200</td>\n",
              "      <td>0.542343</td>\n",
              "      <td>0.827760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.518000</td>\n",
              "      <td>0.535012</td>\n",
              "      <td>0.829280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.509400</td>\n",
              "      <td>0.527897</td>\n",
              "      <td>0.830960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.501600</td>\n",
              "      <td>0.521016</td>\n",
              "      <td>0.832360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.493800</td>\n",
              "      <td>0.514351</td>\n",
              "      <td>0.833800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.486600</td>\n",
              "      <td>0.507928</td>\n",
              "      <td>0.834920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.479100</td>\n",
              "      <td>0.501716</td>\n",
              "      <td>0.836400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.472100</td>\n",
              "      <td>0.495706</td>\n",
              "      <td>0.837920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.465500</td>\n",
              "      <td>0.489926</td>\n",
              "      <td>0.838800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.458900</td>\n",
              "      <td>0.484352</td>\n",
              "      <td>0.839880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.452700</td>\n",
              "      <td>0.478973</td>\n",
              "      <td>0.841360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.446500</td>\n",
              "      <td>0.473795</td>\n",
              "      <td>0.842320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.440600</td>\n",
              "      <td>0.468824</td>\n",
              "      <td>0.843080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.435400</td>\n",
              "      <td>0.464023</td>\n",
              "      <td>0.844400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.429700</td>\n",
              "      <td>0.459424</td>\n",
              "      <td>0.845760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.424500</td>\n",
              "      <td>0.454983</td>\n",
              "      <td>0.846840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.419600</td>\n",
              "      <td>0.450724</td>\n",
              "      <td>0.847600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.414400</td>\n",
              "      <td>0.446623</td>\n",
              "      <td>0.848760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.410200</td>\n",
              "      <td>0.442691</td>\n",
              "      <td>0.849840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.405400</td>\n",
              "      <td>0.438919</td>\n",
              "      <td>0.850640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.401300</td>\n",
              "      <td>0.435293</td>\n",
              "      <td>0.851680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.397000</td>\n",
              "      <td>0.431808</td>\n",
              "      <td>0.852520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.393800</td>\n",
              "      <td>0.428470</td>\n",
              "      <td>0.853320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.388800</td>\n",
              "      <td>0.425274</td>\n",
              "      <td>0.853880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.385900</td>\n",
              "      <td>0.422197</td>\n",
              "      <td>0.855280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.382100</td>\n",
              "      <td>0.419274</td>\n",
              "      <td>0.855880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.378800</td>\n",
              "      <td>0.416468</td>\n",
              "      <td>0.856240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.375800</td>\n",
              "      <td>0.413769</td>\n",
              "      <td>0.857360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.372400</td>\n",
              "      <td>0.411206</td>\n",
              "      <td>0.858000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.369500</td>\n",
              "      <td>0.408743</td>\n",
              "      <td>0.858520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.366700</td>\n",
              "      <td>0.406390</td>\n",
              "      <td>0.859000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.363900</td>\n",
              "      <td>0.404153</td>\n",
              "      <td>0.859360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.361400</td>\n",
              "      <td>0.402006</td>\n",
              "      <td>0.859800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.359200</td>\n",
              "      <td>0.399983</td>\n",
              "      <td>0.860040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.356400</td>\n",
              "      <td>0.398054</td>\n",
              "      <td>0.860600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.354800</td>\n",
              "      <td>0.396213</td>\n",
              "      <td>0.861080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.351900</td>\n",
              "      <td>0.394448</td>\n",
              "      <td>0.861240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.349800</td>\n",
              "      <td>0.392800</td>\n",
              "      <td>0.861600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.348400</td>\n",
              "      <td>0.391214</td>\n",
              "      <td>0.862200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.346300</td>\n",
              "      <td>0.389718</td>\n",
              "      <td>0.862680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.344700</td>\n",
              "      <td>0.388311</td>\n",
              "      <td>0.863240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.342900</td>\n",
              "      <td>0.386982</td>\n",
              "      <td>0.863520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.341600</td>\n",
              "      <td>0.385743</td>\n",
              "      <td>0.863920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>0.339900</td>\n",
              "      <td>0.384577</td>\n",
              "      <td>0.864320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.338600</td>\n",
              "      <td>0.383476</td>\n",
              "      <td>0.864720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>0.337300</td>\n",
              "      <td>0.382455</td>\n",
              "      <td>0.864720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.336300</td>\n",
              "      <td>0.381510</td>\n",
              "      <td>0.864880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>0.335100</td>\n",
              "      <td>0.380634</td>\n",
              "      <td>0.865040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>0.334200</td>\n",
              "      <td>0.379822</td>\n",
              "      <td>0.865200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>0.333000</td>\n",
              "      <td>0.379086</td>\n",
              "      <td>0.865320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>0.332400</td>\n",
              "      <td>0.378416</td>\n",
              "      <td>0.865360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>0.331400</td>\n",
              "      <td>0.377812</td>\n",
              "      <td>0.865440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>0.330700</td>\n",
              "      <td>0.377276</td>\n",
              "      <td>0.865680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>0.330200</td>\n",
              "      <td>0.376797</td>\n",
              "      <td>0.865840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>0.329800</td>\n",
              "      <td>0.376396</td>\n",
              "      <td>0.865840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>0.329200</td>\n",
              "      <td>0.376049</td>\n",
              "      <td>0.865920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>0.328300</td>\n",
              "      <td>0.375764</td>\n",
              "      <td>0.865920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38500</td>\n",
              "      <td>0.328900</td>\n",
              "      <td>0.375546</td>\n",
              "      <td>0.865960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>0.328100</td>\n",
              "      <td>0.375390</td>\n",
              "      <td>0.866040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39500</td>\n",
              "      <td>0.328100</td>\n",
              "      <td>0.375296</td>\n",
              "      <td>0.866040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>0.328300</td>\n",
              "      <td>0.375266</td>\n",
              "      <td>0.866040</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='98' max='98' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [98/98 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-16 14:49:16,135] Trial 5 finished with value: 0.86604 and parameters: {'learning_rate': 1.4724660907331185e-05, 'batch_size': 256}. Best is trial 1 with value: 0.88984.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3000' max='40000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3000/40000 00:52 < 10:49, 56.98 it/s, Epoch 7/103]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.338900</td>\n",
              "      <td>0.282675</td>\n",
              "      <td>0.886200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.154500</td>\n",
              "      <td>0.328608</td>\n",
              "      <td>0.876320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.094900</td>\n",
              "      <td>0.392650</td>\n",
              "      <td>0.869720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.056400</td>\n",
              "      <td>0.480701</td>\n",
              "      <td>0.863560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.029400</td>\n",
              "      <td>0.572586</td>\n",
              "      <td>0.859320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.017600</td>\n",
              "      <td>0.672226</td>\n",
              "      <td>0.854560</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [391/391 00:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-06-16 14:50:13,327] Trial 6 finished with value: 0.8862 and parameters: {'learning_rate': 0.0026832271576898293, 'batch_size': 64}. Best is trial 1 with value: 0.88984.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best trial's hyperparameters and objective value\n",
        "best_trial = study.best_trial\n",
        "print(f\"Best trial (number {best_trial.number}):\")\n",
        "print(f\"  Value: {best_trial.value}\")\n",
        "print(f\"  Params: {best_trial.params}\")\n",
        "\n",
        "# Print all trials' hyperparameters and objective values\n",
        "print(\"\\nAll trials:\")\n",
        "for trial in study.trials:\n",
        "    print(f\"  Trial {trial.number}:\")\n",
        "    print(f\"    Value: {trial.value}\")\n",
        "    print(f\"    Params: {trial.params}\")"
      ],
      "metadata": {
        "id": "LrIitj_Lo6CX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "990a8e26-3ac1-4463-d610-dfe0ad633f9e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial (number 1):\n",
            "  Value: 0.88984\n",
            "  Params: {'learning_rate': 4.671898970848419e-05, 'batch_size': 64}\n",
            "\n",
            "All trials:\n",
            "  Trial 0:\n",
            "    Value: 0.88284\n",
            "    Params: {'learning_rate': 0.0028405788477162846, 'batch_size': 16}\n",
            "  Trial 1:\n",
            "    Value: 0.88984\n",
            "    Params: {'learning_rate': 4.671898970848419e-05, 'batch_size': 64}\n",
            "  Trial 2:\n",
            "    Value: 0.79016\n",
            "    Params: {'learning_rate': 3.400610504968987e-06, 'batch_size': 256}\n",
            "  Trial 3:\n",
            "    Value: 0.79576\n",
            "    Params: {'learning_rate': 7.079172599523785e-06, 'batch_size': 128}\n",
            "  Trial 4:\n",
            "    Value: 0.87576\n",
            "    Params: {'learning_rate': 0.006035009622352109, 'batch_size': 64}\n",
            "  Trial 5:\n",
            "    Value: 0.86604\n",
            "    Params: {'learning_rate': 1.4724660907331185e-05, 'batch_size': 256}\n",
            "  Trial 6:\n",
            "    Value: 0.8862\n",
            "    Params: {'learning_rate': 0.0026832271576898293, 'batch_size': 64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna.visualization as vis\n",
        "fig = vis.plot_parallel_coordinate(study)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "_anVhIp6o_rt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "4403f1b8-e6bc-43ea-f67f-3fb570bdcfca"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"f7f628db-8c2c-4ebb-b8bc-ac1383de5213\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f7f628db-8c2c-4ebb-b8bc-ac1383de5213\")) {                    Plotly.newPlot(                        \"f7f628db-8c2c-4ebb-b8bc-ac1383de5213\",                        [{\"dimensions\":[{\"label\":\"Objective Value\",\"range\":[0.79016,0.88984],\"values\":[0.88284,0.88984,0.87576,0.8862,0.79576,0.79016,0.86604]},{\"label\":\"batch_size\",\"range\":[0,3],\"ticktext\":[\"16\",\"64\",\"128\",\"256\"],\"tickvals\":[0,1,2,3],\"values\":[0,1,1,1,2,3,3]},{\"label\":\"learning_rate\",\"range\":[-5.468443107917203,-2.219322033117404],\"ticktext\":[\"3.4e-06\",\"1e-05\",\"0.0001\",\"0.001\",\"0.00604\"],\"tickvals\":[-5.468443107917203,-5,-4,-3,-2.219322033117404],\"values\":[-2.546593151236983,-4.33050655735059,-2.219322033117404,-2.5713425591002634,-5.150017498872315,-5.468443107917203,-4.831954697747692]}],\"labelangle\":30,\"labelside\":\"bottom\",\"line\":{\"color\":[0.88284,0.88984,0.87576,0.8862,0.79576,0.79016,0.86604],\"colorbar\":{\"title\":{\"text\":\"Objective Value\"}},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"reversescale\":false,\"showscale\":true},\"type\":\"parcoords\"}],                        {\"title\":{\"text\":\"Parallel Coordinate Plot\"},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f7f628db-8c2c-4ebb-b8bc-ac1383de5213');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = vis.plot_slice(study)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "MXkWawHrpABg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "5acb5bdf-b087-42db-cfec-9d797c5c29a6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"caf5cbee-2175-40f4-ba70-e05913d163b6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"caf5cbee-2175-40f4-ba70-e05913d163b6\")) {                    Plotly.newPlot(                        \"caf5cbee-2175-40f4-ba70-e05913d163b6\",                        [{\"marker\":{\"color\":[0,1,2,3,4,5,6],\"colorbar\":{\"title\":{\"text\":\"Trial\"},\"x\":1.0,\"xpad\":40},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"line\":{\"color\":\"Grey\",\"width\":0.5},\"showscale\":true},\"mode\":\"markers\",\"name\":\"Feasible Trial\",\"showlegend\":false,\"x\":[16,64,256,128,64,256,64],\"y\":[0.88284,0.88984,0.79016,0.79576,0.87576,0.86604,0.8862],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":[0,1,2,3,4,5,6],\"colorbar\":{\"title\":{\"text\":\"Trial\"},\"x\":1.0,\"xpad\":40},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"line\":{\"color\":\"Grey\",\"width\":0.5},\"showscale\":false},\"mode\":\"markers\",\"name\":\"Feasible Trial\",\"showlegend\":false,\"x\":[0.0028405788477162846,4.671898970848419e-05,3.400610504968987e-06,7.079172599523785e-06,0.006035009622352109,1.4724660907331185e-05,0.0026832271576898293],\"y\":[0.88284,0.88984,0.79016,0.79576,0.87576,0.86604,0.8862],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"batch_size\"},\"type\":\"category\",\"categoryorder\":\"array\",\"categoryarray\":[16,64,128,256]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Objective Value\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"learning_rate\"},\"type\":\"log\"},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"matches\":\"y\",\"showticklabels\":false},\"title\":{\"text\":\"Slice Plot\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('caf5cbee-2175-40f4-ba70-e05913d163b6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = vis.plot_contour(study, params=['learning_rate', 'batch_size'])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "S3n882-IpBTy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "4d8f66ec-f027-4432-9fa9-754cea392748"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"2a0ed8b5-a774-4026-ac7d-dd3c09fae736\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2a0ed8b5-a774-4026-ac7d-dd3c09fae736\")) {                    Plotly.newPlot(                        \"2a0ed8b5-a774-4026-ac7d-dd3c09fae736\",                        [{\"colorbar\":{\"title\":{\"text\":\"Objective Value\"}},\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"connectgaps\":true,\"contours\":{\"coloring\":\"heatmap\"},\"hoverinfo\":\"none\",\"line\":{\"smoothing\":1.3},\"reversescale\":false,\"x\":[4.0,16,64,128,256,268.0],\"y\":[2.339380353673291e-06,3.400610504968987e-06,7.079172599523785e-06,1.4724660907331185e-05,4.671898970848419e-05,0.0026832271576898293,0.0028405788477162846,0.006035009622352109,0.008772714999993375],\"z\":[[null,null,null,null,null,null],[null,null,null,null,0.79016,null],[null,null,null,0.79576,null,null],[null,null,null,null,0.86604,null],[null,null,0.88984,null,null,null],[null,null,0.8862,null,null,null],[null,0.88284,null,null,null,null],[null,null,0.87576,null,null,null],[null,null,null,null,null,null]],\"type\":\"contour\"},{\"marker\":{\"color\":\"black\",\"line\":{\"color\":\"Gray\",\"width\":2.0}},\"mode\":\"markers\",\"name\":\"Feasible Trial\",\"showlegend\":false,\"x\":[16,64,256,128,64,256,64],\"y\":[0.0028405788477162846,4.671898970848419e-05,3.400610504968987e-06,7.079172599523785e-06,0.006035009622352109,1.4724660907331185e-05,0.0026832271576898293],\"type\":\"scatter\"},{\"marker\":{\"color\":\"#cccccc\",\"line\":{\"color\":\"Gray\",\"width\":2.0}},\"mode\":\"markers\",\"name\":\"Infeasible Trial\",\"showlegend\":false,\"x\":[],\"y\":[],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Contour Plot\"},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"title\":{\"text\":\"batch_size\"},\"range\":[4.0,268.0]},\"yaxis\":{\"title\":{\"text\":\"learning_rate\"},\"range\":[-5.630899161657193,-2.056865979377414],\"type\":\"log\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2a0ed8b5-a774-4026-ac7d-dd3c09fae736');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = vis.plot_optimization_history(study)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "AARiYzlbpCy9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "deb39b4b-1cb8-4dd4-d0ed-439cd24021b8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"74214753-f42a-49f0-9c24-baad76f8ca45\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"74214753-f42a-49f0-9c24-baad76f8ca45\")) {                    Plotly.newPlot(                        \"74214753-f42a-49f0-9c24-baad76f8ca45\",                        [{\"mode\":\"markers\",\"name\":\"Objective Value\",\"x\":[0,1,2,3,4,5,6],\"y\":[0.88284,0.88984,0.79016,0.79576,0.87576,0.86604,0.8862],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Best Value\",\"x\":[0,1,2,3,4,5,6],\"y\":[0.88284,0.88984,0.88984,0.88984,0.88984,0.88984,0.88984],\"type\":\"scatter\"},{\"marker\":{\"color\":\"#cccccc\"},\"mode\":\"markers\",\"name\":\"Infeasible Trial\",\"showlegend\":false,\"x\":[],\"y\":[],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Optimization History Plot\"},\"xaxis\":{\"title\":{\"text\":\"Trial\"}},\"yaxis\":{\"title\":{\"text\":\"Objective Value\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('74214753-f42a-49f0-9c24-baad76f8ca45');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4. Results and summary\n",
        "\n",
        "### 4.1 Corpus insights\n",
        "\n",
        "Large Movie Review Dataset. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. The model predict whether a comment is positive or negative based on the training set, which is evaluated on the test set.\n",
        "Analysing comments is very importnat to due the existense of abundance comments, which is not possibel to be handled by human. THe automatic evaluation of comments allows us to evaluate and rate each case based on the comments and opinions automatically in a real-time. As the language is processing over time, it is of great importance to develop the model and increase the parameters over time as the available comments, words, expressions, and advanced technology increases to achieve better results.\n",
        "\n",
        "### 4.2 Results\n",
        "\n",
        "My model inspired by the exercise 6 in the course to facilitate th programming and avoid mistakes. The model was able to achieve the accuracy of 89% ('learning_rate': 0.000828301723840474, 'batch_size': 128) while in https://doi.org/10.32604/jai.2023.045617, an impressive 93.74% accuracy on the IMDb Dataset has been avhievable recently using Extra-Long Neural Network (XLNet).\n",
        "THe higher values achieve almost with lower learning rate but higher batch size huperparameters.\n",
        "\n",
        "In another study in 2020, 91% was achievable in Sentiment analysis on IMDB using lexicon and neural networks.(https://doi.org/10.1007/s42452-019-1926-x). Almost the same value was achievable in the very new publication in 2024 of What's Next?: Exploring Machine Learning-Based Approaches to Content Suggestions Using IMDb Movie Reviews (https://doi.org10.1109/ICEIC61013.2024.10457130). HOwever, achieving an impressive 93.74% accuracy on the IMDb Dataset is seeable in Opinion Mining on Movie Reviews Based on Deep Learning Models (https://doi.org/10.32604/jai.2023.045617).\n",
        "\n",
        "### 4.3 Relation to state of the art\n",
        "\n",
        "As could be seen, my performance is close to the state of the art results. However, achieving 93% accuracy is very difficult which was able to be reached by using extra-long neural network requiring a huge amount of processing and time (XLNet is an advanced method that combines the advantages of Autoregressive and Autoencoding methods through a technique called permutation language modeling. The neural architecture of XLNet is specifically designed to perform better at Autoregressive tasks, such as Transformer-XL and the carefully crafted two-stream attention mechanism.).\"Although XLNet\n",
        "performed better in many ways, its longer training time may be an issue in circumstances when we need faster results but have limited resources or time to train the model\"\n",
        "However, my model seems to have an acceptable accuracy which is very usefull and applicable for general use unless the sufficient timea and processing equipments are available or needed.\n",
        "---\n",
        "\n",
        "## 5. Bonus Task (optional)\n",
        "\n",
        "### 5.1. Annotating out-of-domain documents\n",
        "\n",
        "(Briefly describe the chosen out-of-domain documents)\n",
        "\n",
        "(Briefly describe the process of annotation)\n",
        "\n",
        "### 5.2 Conversion into dataset"
      ],
      "metadata": {
        "id": "x7ylOS8FdYZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to convert the annotations into a dataset here"
      ],
      "metadata": {
        "id": "32DU04FndRdM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3. Model evaluation on out-of-domain test set"
      ],
      "metadata": {
        "id": "4ghO4JemeFKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to evaluate the model on the out-of-domain test set here"
      ],
      "metadata": {
        "id": "9tzYWQ_zeCYp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Bonus task results\n",
        "\n",
        "(Present the results of the evaluation on the out-of-domain test set)\n",
        "\n",
        "### 5.5. Annotated data"
      ],
      "metadata": {
        "id": "8XLZlItdePfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Include your annotated out-of-domain data here"
      ],
      "metadata": {
        "id": "L2YJsiIGeYRe"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}