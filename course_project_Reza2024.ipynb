{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac5ffe4b370c4242b19af1ea3246e91e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b3771fda8d14e89a6dfe1aea19e8a25",
              "IPY_MODEL_d7c92d3c0b674fe8877a19812a7f6533",
              "IPY_MODEL_87ac5465c32c4cbe88bb490004970afe"
            ],
            "layout": "IPY_MODEL_d145fb1f239f46bc9918f612b4c213df"
          }
        },
        "1b3771fda8d14e89a6dfe1aea19e8a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d32db66e9414365b0baa7e5b1ef60d4",
            "placeholder": "​",
            "style": "IPY_MODEL_3310578c12fb487888161218c9566a12",
            "value": "Downloading builder script: 100%"
          }
        },
        "d7c92d3c0b674fe8877a19812a7f6533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07c9b84f2dda47ef8b1c018e98d1637b",
            "max": 4203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62b04e5149ee4920b5080f2596ecc4e6",
            "value": 4203
          }
        },
        "87ac5465c32c4cbe88bb490004970afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a086707682344633be4068035738153d",
            "placeholder": "​",
            "style": "IPY_MODEL_3466c5e1465942aba5a776c6fb104d0f",
            "value": " 4.20k/4.20k [00:00&lt;00:00, 206kB/s]"
          }
        },
        "d145fb1f239f46bc9918f612b4c213df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d32db66e9414365b0baa7e5b1ef60d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3310578c12fb487888161218c9566a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07c9b84f2dda47ef8b1c018e98d1637b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62b04e5149ee4920b5080f2596ecc4e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a086707682344633be4068035738153d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3466c5e1465942aba5a776c6fb104d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Introduction-to-Human-Language-Technology/blob/main/course_project_Reza2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to HLT Project (Template)\n",
        "\n",
        "- Student(s) Name(s): Mohammadreza Akhtari\n",
        "- Date: April 2024\n",
        "- Chosen Corpus: imdb\n",
        "- Contributions (if group project): -\n",
        "\n",
        "### Corpus information\n",
        "\n",
        "- Description of the chosen corpus: Large Movie Review Dataset. This is a dataset for binary sentiment classification. A set of 25,000 highly polar movie reviews for training, and 25,000 for testing.\n",
        "- Paper(s) and other published materials related to the corpus: Maas, Andrew, et al. \"Learning word vectors for sentiment analysis.\" Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies. 2011.\n",
        "- State-of-the-art performance (best published results) on this corpus:"
      ],
      "metadata": {
        "id": "ucyWlC5gbOyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 1. Setup"
      ],
      "metadata": {
        "id": "D5d-9uxrcDY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to install and import libraries etc. here\n",
        "!pip install --quiet datasets transformers[torch] evaluate optuna plotly\n",
        "import datasets\n",
        "from pprint import pprint #pprint => pretty-print"
      ],
      "metadata": {
        "id": "caHHQoqEcG1J",
        "outputId": "a8965adf-dbb1-42a6-ab77-27bbebd948ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m837.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Data download and preprocessing\n",
        "\n",
        "### 2.1. Download the corpus"
      ],
      "metadata": {
        "id": "ovUapilSb8iT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PDx40YyzbGPc",
        "outputId": "5ca54222-fc92-49d4-d519-89089fce5bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Your code to download the corpus here¨\n",
        "dataset = datasets.load_dataset(\"stanfordnlp/imdb\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq9WESKtpumj",
        "outputId": "2e776920-7294-4870-ab0d-1baa3670d056"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': [\"Before watching this film I had very low expectations and went to just see the cars. Eventually I even regretted going for that reason. Plot is almost non-existent. Character development is non-existent. So many clichés and so much jaw-dropping cheesiness existed in the movie that I could only stare and wonder how it was even released. If not for the exotics, I wouldn't have even rated this movie a 1. An attempt at a coherent story line is destroyed by the sheer absurdity of this elite racing cult and the laughable characters that make up its members. In fact, the movie's plot is so predictable and simple-minded that an average child could foretell the majority of the storyline. Bad acting, bad plot, bad jokes, bad movie.<br /><br />Don't see it. Play Gran Turismo HD instead and it'll satiate your thirst for fast sexy cars without leaving a bad aftertaste.\",\n",
              "  \"I had never heard of this flick despite the connection to George Clooney (whose company produced and he appears in a very funny supporting bit) and his Ocean's 11 director Steven Soderbergh. Worse, we picked this up in a discount bin for $4.99 (Canadian dollars at that!) What a grand and pleasant surprise. But then I'm of the opinion that if William H. Macy is in it you can't be disappointed. This was very reminiscent of those Ealing comedies from England in the 1950s. OK, with more profanity. This is an oddball and at times gut-splittingly funny film. The actual heist made me laugh so hard I was crying. Perhaps the funniest use of underpants in movie history. Maybe it was the low expectations I had going in but I watched with a group of people and we had a blast. Best $5 I've spent in ages.\"],\n",
              " 'label': [0, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Preprocessing"
      ],
      "metadata": {
        "id": "cXb7CQNCbZOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for any necessary preprocessing here\n",
        "#This is never a bad idea, datasets may have ordering to them, which is not what we want\n",
        "dataset=dataset.shuffle()\n",
        "#Delete the unlabeled part of the dataset, we don't need it for anything\n",
        "del dataset[\"unsupervised\"]"
      ],
      "metadata": {
        "id": "RO5BXCuRbYKr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.feature_extraction\n",
        "vectorizer=sklearn.feature_extraction.text.CountVectorizer(binary=True, max_features=25000)\n",
        "\n",
        "#get a list of all texts from the training data\n",
        "texts=[ex[\"text\"] for ex in dataset[\"train\"]]\n",
        "\n",
        "#\"Trains\" the vectorizer, i.e. builds its vocabulary\n",
        "vectorizer.fit(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "OHMjKIAYoK4-",
        "outputId": "19fbba4b-fbf0-4116-ba69-3904adb96332"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(binary=True, max_features=25000)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True, max_features=25000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True, max_features=25000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_example(ex):\n",
        "    #because the vectorizer expects a list/iterable over inputs, not one input\n",
        "    vectorized=vectorizer.transform([ex[\"text\"]])\n",
        "    #.nonzero gives a pair of (rows,columns), we want the columns\n",
        "    non_zero_features=vectorized.nonzero()[1]\n",
        "    #feature index 0 will have a special meaning\n",
        "    # so let us produce it by adding +1 to everything\n",
        "    non_zero_features+=1\n",
        "    return {\"input_ids\":non_zero_features}\n",
        "\n",
        "vectorized=vectorize_example(dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "X-n6yer9orhq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can map back to vocabulary and check that everything works\n",
        "# vectorizer.vocabulary_ is a dictionary {key:word, value:idx}\n",
        "\n",
        "idx2word=dict((i,w) for (w,i) in vectorizer.vocabulary_.items()) #inverse the vocab dictionary\n",
        "words=[]\n",
        "for idx in vectorized[\"input_ids\"]:\n",
        "    ## It is easy to forgot we moved all by +1\n",
        "    words.append(idx2word[idx-1])\n",
        "#This is now the bag of words representation of the document\n",
        "pprint(\", \".join(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWkg4om7otk7",
        "outputId": "b9b0bea5-4cc6-4f85-c318-d7b3a00858bc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('absurdity, acting, aftertaste, almost, an, and, at, attempt, average, bad, '\n",
            " 'before, br, by, cars, character, characters, cheesiness, child, clichés, '\n",
            " 'coherent, could, cult, destroyed, development, don, dropping, elite, even, '\n",
            " 'eventually, existed, existent, expectations, fact, fast, film, for, going, '\n",
            " 'had, have, hd, how, if, in, instead, is, it, its, jaw, jokes, just, '\n",
            " 'laughable, leaving, line, ll, low, majority, make, many, members, minded, '\n",
            " 'movie, much, non, not, of, only, play, plot, predictable, racing, rated, '\n",
            " 'reason, regretted, released, see, sexy, sheer, simple, so, stare, story, '\n",
            " 'storyline, that, the, thirst, this, to, up, very, was, watching, went, '\n",
            " 'without, wonder, wouldn, your')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenizer to the whole dataset using .map()\n",
        "dataset_tokenized = dataset.map(vectorize_example,num_proc=4)\n",
        "pprint(dataset_tokenized[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84KNLNkApmNk",
        "outputId": "6eadb844-ce95-47f1-af5f-c1da0cd8431f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [374,\n",
            "               484,\n",
            "               698,\n",
            "               905,\n",
            "               1035,\n",
            "               1066,\n",
            "               1566,\n",
            "               1607,\n",
            "               1722,\n",
            "               1830,\n",
            "               2157,\n",
            "               2840,\n",
            "               3258,\n",
            "               3529,\n",
            "               3779,\n",
            "               3789,\n",
            "               3879,\n",
            "               3934,\n",
            "               4215,\n",
            "               4358,\n",
            "               5123,\n",
            "               5455,\n",
            "               6121,\n",
            "               6177,\n",
            "               6695,\n",
            "               6913,\n",
            "               7295,\n",
            "               7771,\n",
            "               7779,\n",
            "               7918,\n",
            "               7920,\n",
            "               7941,\n",
            "               8099,\n",
            "               8227,\n",
            "               8451,\n",
            "               8767,\n",
            "               9567,\n",
            "               10018,\n",
            "               10262,\n",
            "               10294,\n",
            "               10824,\n",
            "               11010,\n",
            "               11216,\n",
            "               11565,\n",
            "               11851,\n",
            "               11881,\n",
            "               11889,\n",
            "               11969,\n",
            "               12093,\n",
            "               12203,\n",
            "               12701,\n",
            "               12790,\n",
            "               13010,\n",
            "               13093,\n",
            "               13261,\n",
            "               13483,\n",
            "               13486,\n",
            "               13617,\n",
            "               14012,\n",
            "               14219,\n",
            "               14638,\n",
            "               14658,\n",
            "               15164,\n",
            "               15209,\n",
            "               15413,\n",
            "               15511,\n",
            "               16638,\n",
            "               16686,\n",
            "               16998,\n",
            "               17651,\n",
            "               17804,\n",
            "               17897,\n",
            "               18127,\n",
            "               18192,\n",
            "               19589,\n",
            "               19791,\n",
            "               19892,\n",
            "               20181,\n",
            "               20574,\n",
            "               21102,\n",
            "               21309,\n",
            "               21312,\n",
            "               22330,\n",
            "               22335,\n",
            "               22407,\n",
            "               22413,\n",
            "               22599,\n",
            "               23631,\n",
            "               23870,\n",
            "               24232,\n",
            "               24257,\n",
            "               24381,\n",
            "               24629,\n",
            "               24669,\n",
            "               24741,\n",
            "               24906],\n",
            " 'label': 0,\n",
            " 'text': 'Before watching this film I had very low expectations and went to '\n",
            "         'just see the cars. Eventually I even regretted going for that '\n",
            "         'reason. Plot is almost non-existent. Character development is '\n",
            "         'non-existent. So many clichés and so much jaw-dropping cheesiness '\n",
            "         'existed in the movie that I could only stare and wonder how it was '\n",
            "         \"even released. If not for the exotics, I wouldn't have even rated \"\n",
            "         'this movie a 1. An attempt at a coherent story line is destroyed by '\n",
            "         'the sheer absurdity of this elite racing cult and the laughable '\n",
            "         \"characters that make up its members. In fact, the movie's plot is so \"\n",
            "         'predictable and simple-minded that an average child could foretell '\n",
            "         'the majority of the storyline. Bad acting, bad plot, bad jokes, bad '\n",
            "         \"movie.<br /><br />Don't see it. Play Gran Turismo HD instead and \"\n",
            "         \"it'll satiate your thirst for fast sexy cars without leaving a bad \"\n",
            "         'aftertaste.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collator(list_of_examples):\n",
        "    batch={\"labels\":torch.tensor(list(ex[\"label\"] for ex in list_of_examples))} #this is easy, labels are made into a single tensor\n",
        "    #the worse bit is now to pad the examples, as they are of different length\n",
        "    tensors=[]\n",
        "    max_len=max(len(example[\"input_ids\"]) for example in list_of_examples) #this is the longest example in the batch\n",
        "    #everything needs to be padded to fit in length the longest example\n",
        "    #(so we can build a single tensor out of it)\n",
        "    for example in list_of_examples:\n",
        "        ids=torch.tensor(example[\"input_ids\"]) #pick the input ids\n",
        "        # pad(what,(from_left, from_right)) <- this is how we call the stock pad function\n",
        "        padded=torch.nn.functional.pad(ids,(0,max_len-ids.shape[0])) #pad by max - current length, pads with zero by default\n",
        "        tensors.append(padded) #accumulated the padded ids\n",
        "    batch[\"input_ids\"]=torch.vstack(tensors) #now that we have all of them the same length, a simple vstack() stacks them up\n",
        "    return batch #...and that's all there is to it\n"
      ],
      "metadata": {
        "id": "Dm1WR3ggsKGs"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Machine learning model\n",
        "\n",
        "### 3.1. Model training"
      ],
      "metadata": {
        "id": "F1ntHh_JbrAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to train the machine learning model on the training set and evaluate the performance on the validation set here\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "# A model wants a config, I can simply inherit from the base\n",
        "# class for pretrained configs\n",
        "class MLPConfig(transformers.PretrainedConfig):\n",
        "    pass\n",
        "\n",
        "# This is the model\n",
        "class MLP(transformers.PreTrainedModel):\n",
        "\n",
        "    config_class=MLPConfig\n",
        "\n",
        "    # In the initialization method, one instantiates the layers\n",
        "    # these will be, for the most part the trained parameters of the model\n",
        "    def __init__(self,config):\n",
        "        super().__init__(config)\n",
        "        self.vocab_size=config.vocab_size #embedding matrix row count\n",
        "        # Build and initialize embedding of vocab size +1 x hidden size (+1 because of the padding index 0!)\n",
        "        self.embedding=torch.nn.Embedding(num_embeddings=self.vocab_size+1,embedding_dim=config.hidden_size,padding_idx=0)\n",
        "        # Normally you would not initialize these yourself, but I have my reasons here ;)\n",
        "        torch.nn.init.uniform_(self.embedding.weight.data,-0.001,0.001) #initialize the embeddings with small random values\n",
        "        # Note! This is quite clever and keeps the embedding for 0, the padding, pure zeros\n",
        "        # This takes care of the lower half of the network, now the upper half\n",
        "        # Output layer: hidden size x output size\n",
        "        self.output=torch.nn.Linear(in_features=config.hidden_size,out_features=config.nlabels)\n",
        "        # Now we have the parameters of the model\n",
        "\n",
        "\n",
        "    # The computation of the model is put into the forward() function\n",
        "    # it receives a batch of data and optionally the correct `labels`\n",
        "    #\n",
        "    # If given `labels` it returns (loss,output)\n",
        "    # if not, then it returns (output,)\n",
        "    def forward(self,input_ids,labels=None): #nevermind the attention_mask, its time will come, data collator insists on adding it\n",
        "        #1) sum up the embeddings of the items\n",
        "        embedded=self.embedding(input_ids) #(batch,ids)->(batch,ids,embedding_dim)\n",
        "        # Since the Embedding keeps the first row of the matrix pure zeros, we don't need to worry about the padding\n",
        "        # so next we sum the embeddings across the word dimension\n",
        "        # (batch,ids,embedding_dim) -> (batch,embedding_dim)\n",
        "        embedded_summed=torch.sum(embedded,dim=1)\n",
        "\n",
        "        #2) apply non-linearity\n",
        "        # (batch,embedding_dim) -> (batch,embedding_dim)\n",
        "\n",
        "        #### MODIFIED HERE FOR EXERCISE 5 -> commented out\n",
        "        ####projected=torch.tanh(embedded_summed) #Note how non-linearity is applied here and not when configuring the layer in __init__()\n",
        "\n",
        "        #3) and now apply the upper, output layer of the network\n",
        "        # (batch,embedding_dim) -> (batch, num_of_classes i.e. 2 in our case)\n",
        "\n",
        "        #### MODIFIED HERE FOR EXERCISE 5 -> base it off embedded_summed\n",
        "        ##### OLD: logits=self.output(projected)\n",
        "        logits=self.output(embedded_summed)\n",
        "\n",
        "        # ...and that's all there is to it!\n",
        "\n",
        "        #print(\"input_ids.shape\",input_ids.shape)\n",
        "        #print(\"embedded.shape\",embedded.shape)\n",
        "        #print(\"embedded_summed.shape\",embedded_summed.shape)\n",
        "        #print(\"projected.shape\",projected.shape)\n",
        "        #print(\"logits.shape\",logits.shape)\n",
        "\n",
        "        # We have labels, so we ought to calculate the loss\n",
        "        if labels is not None:\n",
        "            loss=torch.nn.CrossEntropyLoss() #This loss is meant for classification, so let's use it\n",
        "            # You run it as loss(model_output,correct_labels)\n",
        "            return (loss(logits,labels),logits)\n",
        "        else:\n",
        "            # No labels, so just return the logits\n",
        "            return (logits,)\n",
        "\n",
        "# Configure the model:\n",
        "#   these parameters are used in the model's __init__()\n",
        "\n",
        "\n",
        "####### CHANGED HERE FOR EXERCISE 5, hidden_size from 20 to 1\n",
        "mlp_config=MLPConfig(vocab_size=len(vectorizer.vocabulary_),hidden_size=1,nlabels=2)\n"
      ],
      "metadata": {
        "id": "Hs2Bf49zbn5C"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And we can make a model\n",
        "mlp=MLP(mlp_config)\n",
        "fake_batch=collator([dataset_tokenized[\"train\"][0],dataset_tokenized[\"train\"][1]])\n",
        "mlp(**fake_batch) #** expands input_ids and labels as parameters of the call"
      ],
      "metadata": {
        "id": "X3QoSj16sUgI",
        "outputId": "21ae8ada-53b7-41df-b0a0-b72838abef0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.7372, grad_fn=<NllLossBackward0>),\n",
              " tensor([[0.7065, 0.1133],\n",
              "         [0.7142, 0.1188]], grad_fn=<AddmmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training arguments\n",
        "# their names are mostly self-explanatory\n",
        "trainer_args = transformers.TrainingArguments(\n",
        "    \"mlp_checkpoints\", #save checkpoints here\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_steps=500,\n",
        "    learning_rate=1e-4, #learning rate of the gradient descent\n",
        "    max_steps=20000,\n",
        "    load_best_model_at_end=True,\n",
        "    per_device_train_batch_size=128\n",
        ")\n",
        "\n",
        "pprint(trainer_args)"
      ],
      "metadata": {
        "id": "PqkMZLKrsX7B",
        "outputId": "ca978466-9ab3-41db-955f-4996d3d2e4ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=mlp_checkpoints/runs/May06_11-37-22_60787fc1fe02,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=20000,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=mlp_checkpoints,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=128,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=mlp_checkpoints,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Hyperparameter optimization"
      ],
      "metadata": {
        "id": "nlO8RVuHcmAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for hyperparameter optimization here\n"
      ],
      "metadata": {
        "id": "IzDrTDd0cWOG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Evaluation on test set"
      ],
      "metadata": {
        "id": "1EzCYTnfcrvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to evaluate the final model on the test set here\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_accuracy(outputs_and_labels):\n",
        "    outputs, labels = outputs_and_labels\n",
        "    predictions = np.argmax(outputs, axis=-1) #pick the index of the \"winning\" label\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "BG7s-yr6crGF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ac5ffe4b370c4242b19af1ea3246e91e",
            "1b3771fda8d14e89a6dfe1aea19e8a25",
            "d7c92d3c0b674fe8877a19812a7f6533",
            "87ac5465c32c4cbe88bb490004970afe",
            "d145fb1f239f46bc9918f612b4c213df",
            "7d32db66e9414365b0baa7e5b1ef60d4",
            "3310578c12fb487888161218c9566a12",
            "07c9b84f2dda47ef8b1c018e98d1637b",
            "62b04e5149ee4920b5080f2596ecc4e6",
            "a086707682344633be4068035738153d",
            "3466c5e1465942aba5a776c6fb104d0f"
          ]
        },
        "outputId": "aef54a2d-5c08-41d1-f631-8935e857526a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac5ffe4b370c4242b19af1ea3246e91e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a new model\n",
        "mlp = MLP(mlp_config)\n",
        "\n",
        "\n",
        "# Argument gives the number of steps of patience before early stopping\n",
        "# i.e. training is stopped when the evaluation loss fails to improve\n",
        "# certain number of times\n",
        "early_stopping = transformers.EarlyStoppingCallback(5)\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=mlp,\n",
        "    args=trainer_args,\n",
        "    train_dataset=dataset_tokenized[\"train\"],\n",
        "    eval_dataset=dataset_tokenized[\"test\"].select(range(1000)), #make a smaller subset to evaluate on\n",
        "    compute_metrics=compute_accuracy,\n",
        "    data_collator=collator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# FINALLY!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "gmv5cOMast4l",
        "outputId": "0f15ffee-1645-49b9-bddc-8c5f1a9e157d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 942
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13000/20000 07:14 < 03:54, 29.91 it/s, Epoch 66/103]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.674800</td>\n",
              "      <td>0.650864</td>\n",
              "      <td>0.763000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.603600</td>\n",
              "      <td>0.580271</td>\n",
              "      <td>0.828000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.521700</td>\n",
              "      <td>0.514105</td>\n",
              "      <td>0.836000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.452900</td>\n",
              "      <td>0.462030</td>\n",
              "      <td>0.848000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.399900</td>\n",
              "      <td>0.423566</td>\n",
              "      <td>0.859000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.358000</td>\n",
              "      <td>0.393951</td>\n",
              "      <td>0.861000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.326300</td>\n",
              "      <td>0.371326</td>\n",
              "      <td>0.867000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.300300</td>\n",
              "      <td>0.354355</td>\n",
              "      <td>0.872000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.278400</td>\n",
              "      <td>0.340955</td>\n",
              "      <td>0.874000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.260900</td>\n",
              "      <td>0.330168</td>\n",
              "      <td>0.874000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.246500</td>\n",
              "      <td>0.322141</td>\n",
              "      <td>0.874000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.232900</td>\n",
              "      <td>0.315534</td>\n",
              "      <td>0.878000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.220500</td>\n",
              "      <td>0.310701</td>\n",
              "      <td>0.881000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.211700</td>\n",
              "      <td>0.307083</td>\n",
              "      <td>0.879000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.201700</td>\n",
              "      <td>0.304112</td>\n",
              "      <td>0.878000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.194200</td>\n",
              "      <td>0.301799</td>\n",
              "      <td>0.879000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.186700</td>\n",
              "      <td>0.300050</td>\n",
              "      <td>0.882000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.179400</td>\n",
              "      <td>0.299230</td>\n",
              "      <td>0.882000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.174000</td>\n",
              "      <td>0.298550</td>\n",
              "      <td>0.882000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.168400</td>\n",
              "      <td>0.298352</td>\n",
              "      <td>0.883000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.162600</td>\n",
              "      <td>0.298312</td>\n",
              "      <td>0.882000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.158600</td>\n",
              "      <td>0.298592</td>\n",
              "      <td>0.882000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.153600</td>\n",
              "      <td>0.299071</td>\n",
              "      <td>0.883000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.150500</td>\n",
              "      <td>0.299644</td>\n",
              "      <td>0.884000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.147100</td>\n",
              "      <td>0.300307</td>\n",
              "      <td>0.883000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.141900</td>\n",
              "      <td>0.301069</td>\n",
              "      <td>0.882000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=13000, training_loss=0.2733456326998197, metrics={'train_runtime': 435.0373, 'train_samples_per_second': 5884.553, 'train_steps_per_second': 45.973, 'total_flos': 17285442432.0, 'train_loss': 0.2733456326998197, 'epoch': 66.3265306122449})"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate(dataset_tokenized[\"test\"])\n",
        "\n",
        "print(eval_results)"
      ],
      "metadata": {
        "id": "3sE_hfHWs1Lu",
        "outputId": "a49125cf-84bc-4204-d1e6-561ea2a783cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3125/3125 00:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.27793562412261963, 'eval_accuracy': 0.88948, 'eval_runtime': 21.9119, 'eval_samples_per_second': 1140.935, 'eval_steps_per_second': 142.617, 'epoch': 66.3265306122449}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # Define the search space for hyperparameters\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-3, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 64, 128, 256])\n",
        "\n",
        "\n",
        "    trainer_args = transformers.TrainingArguments(\n",
        "        \"mlp_checkpoints\", #save checkpoints here\n",
        "        evaluation_strategy=\"steps\",\n",
        "        logging_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        logging_steps=500,\n",
        "        learning_rate=learning_rate, #learning rate of the gradient descent\n",
        "        max_steps=10000, #I will keep this small, no time to wait too long :)\n",
        "        load_best_model_at_end=True,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    mlp = MLP(mlp_config)\n",
        "\n",
        "    trainer = transformers.Trainer(\n",
        "        model=mlp,\n",
        "        args=trainer_args,\n",
        "        train_dataset=dataset_tokenized[\"train\"],\n",
        "        eval_dataset=dataset_tokenized[\"test\"].select(range(1000)), #make a smaller subset to evaluate on\n",
        "        compute_metrics=compute_accuracy,\n",
        "        data_collator=collator,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Train the model and get the best validation loss\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "    return eval_results[\"eval_accuracy\"] #let's try to maximize accuracy\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=7) #I will keep this small, no time to wait :)"
      ],
      "metadata": {
        "id": "N1ZJulkytCCf",
        "outputId": "e02454d3-9ad2-42c4-db18-a4cd99a32c5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-06 11:56:49,541] A new study created in memory with name: no-name-904d54ca-a712-4c53-9966-aa122dffb979\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5000/10000 02:21 < 02:21, 35.32 it/s, Epoch 25/52]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.511900</td>\n",
              "      <td>0.425763</td>\n",
              "      <td>0.855000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.322800</td>\n",
              "      <td>0.347142</td>\n",
              "      <td>0.873000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.247700</td>\n",
              "      <td>0.317156</td>\n",
              "      <td>0.879000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.205000</td>\n",
              "      <td>0.305269</td>\n",
              "      <td>0.880000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.175600</td>\n",
              "      <td>0.301831</td>\n",
              "      <td>0.877000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.153100</td>\n",
              "      <td>0.302249</td>\n",
              "      <td>0.882000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.135900</td>\n",
              "      <td>0.305305</td>\n",
              "      <td>0.883000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.122000</td>\n",
              "      <td>0.311477</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.109300</td>\n",
              "      <td>0.316243</td>\n",
              "      <td>0.877000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.322844</td>\n",
              "      <td>0.877000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-06 11:59:12,032] Trial 0 finished with value: 0.877 and parameters: {'learning_rate': 0.0002814569334793928, 'batch_size': 128}. Best is trial 0 with value: 0.877.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 01:10, Epoch 6/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.691200</td>\n",
              "      <td>0.683589</td>\n",
              "      <td>0.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.679600</td>\n",
              "      <td>0.674352</td>\n",
              "      <td>0.627000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.669500</td>\n",
              "      <td>0.666163</td>\n",
              "      <td>0.691000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.659700</td>\n",
              "      <td>0.658769</td>\n",
              "      <td>0.731000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.652700</td>\n",
              "      <td>0.651901</td>\n",
              "      <td>0.754000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.644700</td>\n",
              "      <td>0.645556</td>\n",
              "      <td>0.765000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.637400</td>\n",
              "      <td>0.639870</td>\n",
              "      <td>0.775000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.631500</td>\n",
              "      <td>0.634539</td>\n",
              "      <td>0.786000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.623400</td>\n",
              "      <td>0.629706</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.618800</td>\n",
              "      <td>0.625376</td>\n",
              "      <td>0.801000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.615900</td>\n",
              "      <td>0.621507</td>\n",
              "      <td>0.804000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.611300</td>\n",
              "      <td>0.618064</td>\n",
              "      <td>0.810000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.606100</td>\n",
              "      <td>0.615077</td>\n",
              "      <td>0.809000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.603000</td>\n",
              "      <td>0.612404</td>\n",
              "      <td>0.812000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.601500</td>\n",
              "      <td>0.610337</td>\n",
              "      <td>0.812000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.597900</td>\n",
              "      <td>0.608540</td>\n",
              "      <td>0.810000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.597600</td>\n",
              "      <td>0.607209</td>\n",
              "      <td>0.811000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.595700</td>\n",
              "      <td>0.606191</td>\n",
              "      <td>0.810000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.593800</td>\n",
              "      <td>0.605639</td>\n",
              "      <td>0.810000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.592800</td>\n",
              "      <td>0.605443</td>\n",
              "      <td>0.812000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-06 12:00:23,390] Trial 1 finished with value: 0.812 and parameters: {'learning_rate': 1.0018017519383743e-05, 'batch_size': 16}. Best is trial 0 with value: 0.877.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5500' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5500/10000 04:05 < 03:20, 22.41 it/s, Epoch 56/103]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.535100</td>\n",
              "      <td>0.460294</td>\n",
              "      <td>0.851000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.353800</td>\n",
              "      <td>0.370723</td>\n",
              "      <td>0.865000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.271000</td>\n",
              "      <td>0.332514</td>\n",
              "      <td>0.868000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.223600</td>\n",
              "      <td>0.315012</td>\n",
              "      <td>0.877000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.190500</td>\n",
              "      <td>0.306252</td>\n",
              "      <td>0.880000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.167500</td>\n",
              "      <td>0.303395</td>\n",
              "      <td>0.879000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.148500</td>\n",
              "      <td>0.304775</td>\n",
              "      <td>0.879000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.133600</td>\n",
              "      <td>0.306913</td>\n",
              "      <td>0.878000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.121100</td>\n",
              "      <td>0.310581</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.110900</td>\n",
              "      <td>0.315207</td>\n",
              "      <td>0.873000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.102200</td>\n",
              "      <td>0.320170</td>\n",
              "      <td>0.872000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-06 12:04:29,339] Trial 2 finished with value: 0.879 and parameters: {'learning_rate': 0.0002203441254376891, 'batch_size': 256}. Best is trial 2 with value: 0.879.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 01:18, Epoch 6/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.668200</td>\n",
              "      <td>0.627122</td>\n",
              "      <td>0.779000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.562200</td>\n",
              "      <td>0.514474</td>\n",
              "      <td>0.836000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.452500</td>\n",
              "      <td>0.432669</td>\n",
              "      <td>0.847000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.372700</td>\n",
              "      <td>0.386552</td>\n",
              "      <td>0.860000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.330400</td>\n",
              "      <td>0.354608</td>\n",
              "      <td>0.870000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.303100</td>\n",
              "      <td>0.336449</td>\n",
              "      <td>0.877000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.273300</td>\n",
              "      <td>0.324028</td>\n",
              "      <td>0.873000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.255200</td>\n",
              "      <td>0.312926</td>\n",
              "      <td>0.873000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.245000</td>\n",
              "      <td>0.307644</td>\n",
              "      <td>0.873000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.221600</td>\n",
              "      <td>0.303400</td>\n",
              "      <td>0.878000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.222800</td>\n",
              "      <td>0.299587</td>\n",
              "      <td>0.876000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.217700</td>\n",
              "      <td>0.297151</td>\n",
              "      <td>0.877000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.199800</td>\n",
              "      <td>0.296296</td>\n",
              "      <td>0.883000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.197100</td>\n",
              "      <td>0.294728</td>\n",
              "      <td>0.878000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.198300</td>\n",
              "      <td>0.293833</td>\n",
              "      <td>0.880000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.293525</td>\n",
              "      <td>0.881000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.194800</td>\n",
              "      <td>0.293097</td>\n",
              "      <td>0.882000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.180600</td>\n",
              "      <td>0.292742</td>\n",
              "      <td>0.881000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.180500</td>\n",
              "      <td>0.292914</td>\n",
              "      <td>0.881000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.178500</td>\n",
              "      <td>0.292909</td>\n",
              "      <td>0.882000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-06 12:05:48,705] Trial 3 finished with value: 0.881 and parameters: {'learning_rate': 0.0002711357249711533, 'batch_size': 16}. Best is trial 3 with value: 0.881.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 07:26, Epoch 102/103]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.692300</td>\n",
              "      <td>0.690756</td>\n",
              "      <td>0.509000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.689500</td>\n",
              "      <td>0.688572</td>\n",
              "      <td>0.530000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.686900</td>\n",
              "      <td>0.686539</td>\n",
              "      <td>0.604000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.684500</td>\n",
              "      <td>0.684644</td>\n",
              "      <td>0.665000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.682200</td>\n",
              "      <td>0.682877</td>\n",
              "      <td>0.713000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.680200</td>\n",
              "      <td>0.681234</td>\n",
              "      <td>0.751000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.678300</td>\n",
              "      <td>0.679711</td>\n",
              "      <td>0.768000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.676500</td>\n",
              "      <td>0.678305</td>\n",
              "      <td>0.778000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.674800</td>\n",
              "      <td>0.677015</td>\n",
              "      <td>0.782000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.673400</td>\n",
              "      <td>0.675841</td>\n",
              "      <td>0.787000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.672000</td>\n",
              "      <td>0.674779</td>\n",
              "      <td>0.791000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.670800</td>\n",
              "      <td>0.673831</td>\n",
              "      <td>0.790000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.669700</td>\n",
              "      <td>0.672996</td>\n",
              "      <td>0.793000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.668800</td>\n",
              "      <td>0.672271</td>\n",
              "      <td>0.795000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.668000</td>\n",
              "      <td>0.671660</td>\n",
              "      <td>0.796000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.667200</td>\n",
              "      <td>0.671160</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.666800</td>\n",
              "      <td>0.670771</td>\n",
              "      <td>0.803000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.666400</td>\n",
              "      <td>0.670494</td>\n",
              "      <td>0.805000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.666100</td>\n",
              "      <td>0.670328</td>\n",
              "      <td>0.804000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.665900</td>\n",
              "      <td>0.670272</td>\n",
              "      <td>0.804000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-06 12:13:15,667] Trial 4 finished with value: 0.804 and parameters: {'learning_rate': 1.4507454347811181e-06, 'batch_size': 256}. Best is trial 3 with value: 0.881.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3500' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3500/10000 02:37 < 04:52, 22.26 it/s, Epoch 35/103]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.471200</td>\n",
              "      <td>0.350747</td>\n",
              "      <td>0.860000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.214600</td>\n",
              "      <td>0.303301</td>\n",
              "      <td>0.874000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.142600</td>\n",
              "      <td>0.310203</td>\n",
              "      <td>0.876000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.103200</td>\n",
              "      <td>0.333006</td>\n",
              "      <td>0.869000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.076600</td>\n",
              "      <td>0.359431</td>\n",
              "      <td>0.870000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.058200</td>\n",
              "      <td>0.389073</td>\n",
              "      <td>0.864000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.044600</td>\n",
              "      <td>0.423939</td>\n",
              "      <td>0.859000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-06 12:15:53,470] Trial 5 finished with value: 0.874 and parameters: {'learning_rate': 0.000693695683466865, 'batch_size': 256}. Best is trial 3 with value: 0.881.\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10000/10000 04:09, Epoch 51/52]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.688000</td>\n",
              "      <td>0.683859</td>\n",
              "      <td>0.778000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.677600</td>\n",
              "      <td>0.675433</td>\n",
              "      <td>0.772000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.667700</td>\n",
              "      <td>0.667361</td>\n",
              "      <td>0.777000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.658500</td>\n",
              "      <td>0.659689</td>\n",
              "      <td>0.784000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.649800</td>\n",
              "      <td>0.652523</td>\n",
              "      <td>0.788000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.641500</td>\n",
              "      <td>0.645838</td>\n",
              "      <td>0.785000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.634000</td>\n",
              "      <td>0.639605</td>\n",
              "      <td>0.795000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.626900</td>\n",
              "      <td>0.633885</td>\n",
              "      <td>0.796000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.620400</td>\n",
              "      <td>0.628696</td>\n",
              "      <td>0.797000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.614800</td>\n",
              "      <td>0.623948</td>\n",
              "      <td>0.801000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.609500</td>\n",
              "      <td>0.619774</td>\n",
              "      <td>0.801000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.604800</td>\n",
              "      <td>0.615999</td>\n",
              "      <td>0.804000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.600500</td>\n",
              "      <td>0.612733</td>\n",
              "      <td>0.804000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.597000</td>\n",
              "      <td>0.609899</td>\n",
              "      <td>0.807000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.594000</td>\n",
              "      <td>0.607523</td>\n",
              "      <td>0.807000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.591500</td>\n",
              "      <td>0.605571</td>\n",
              "      <td>0.808000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.589300</td>\n",
              "      <td>0.604067</td>\n",
              "      <td>0.809000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.587900</td>\n",
              "      <td>0.603001</td>\n",
              "      <td>0.809000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.586800</td>\n",
              "      <td>0.602356</td>\n",
              "      <td>0.809000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.586600</td>\n",
              "      <td>0.602143</td>\n",
              "      <td>0.809000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-06 12:20:03,306] Trial 6 finished with value: 0.809 and parameters: {'learning_rate': 1.0068025147504158e-05, 'batch_size': 128}. Best is trial 3 with value: 0.881.\n",
            "[I 2024-05-06 12:20:03,321] A new study created in memory with name: no-name-dafc593b-c24c-40f5-b523-7cb8890bd8a6\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best trial's hyperparameters and objective value\n",
        "best_trial = study.best_trial\n",
        "print(f\"Best trial (number {best_trial.number}):\")\n",
        "print(f\"  Value: {best_trial.value}\")\n",
        "print(f\"  Params: {best_trial.params}\")\n",
        "\n",
        "# Print all trials' hyperparameters and objective values\n",
        "print(\"\\nAll trials:\")\n",
        "for trial in study.trials:\n",
        "    print(f\"  Trial {trial.number}:\")\n",
        "    print(f\"    Value: {trial.value}\")\n",
        "    print(f\"    Params: {trial.params}\")"
      ],
      "metadata": {
        "id": "jKIykFcxtFNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna.visualization as vis\n",
        "fig = vis.plot_parallel_coordinate(study)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "LZskB47ItJEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = vis.plot_slice(study)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "BusLOH2stLU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = vis.plot_optimization_history(study)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "FJ7PEvmgtOaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KDXWm7_Bteyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "def objective(trial):\n",
        "    # Define the search space for hyperparameters\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-3, log=True)\n",
        "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 1, 5)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "\n",
        "    # Load the dataset, tokenizer, and model\n",
        "    dataset = ...\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"your-pretrained-model\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"your-pretrained-model\")\n",
        "\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        eval_steps=100,\n",
        "        disable_tqdm=True,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    # Set up the trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"validation\"],\n",
        "    )\n",
        "\n",
        "    # Train the model and get the best validation loss\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "    return eval_results[\"eval_loss\"]"
      ],
      "metadata": {
        "id": "yqi8yZEKte2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=20)"
      ],
      "metadata": {
        "id": "_w_09NeBtgdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "swrThSa1tm8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna\n",
        "pip install plotly\n"
      ],
      "metadata": {
        "id": "U7_SHKK6tt-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna.visualization as vis\n",
        "\n",
        "fig = vis.plot_parallel_coordinate(study)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "uKK3v0LHt3Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig = vis.plot_slice(study)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "7vCX3Oy_t3Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = vis.plot_contour(study, params=['learning_rate', 'num_train_epochs', 'batch_size'])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "eNiK09FBt3SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = vis.plot_optimization_history(study)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "vwuon8NXt3UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best trial's hyperparameters and objective value\n",
        "best_trial = study.best_trial\n",
        "print(f\"Best trial (number {best_trial.number}):\")\n",
        "print(f\"  Value: {best_trial.value}\")\n",
        "print(f\"  Params: {best_trial.params}\")\n",
        "\n",
        "# Print all trials' hyperparameters and objective values\n",
        "print(\"\\nAll trials:\")\n",
        "for trial in study.trials:\n",
        "    print(f\"  Trial {trial.number}:\")\n",
        "    print(f\"    Value: {trial.value}\")\n",
        "    print(f\"    Params: {trial.params}\")"
      ],
      "metadata": {
        "id": "qNtaoKZYtnAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4. Results and summary\n",
        "\n",
        "### 4.1 Corpus insights\n",
        "\n",
        "(Briefly discuss what you learned about the corpus and its annotation)\n",
        "\n",
        "### 4.2 Results\n",
        "\n",
        "(Briefly summarize your results)\n",
        "\n",
        "### 4.3 Relation to state of the art\n",
        "\n",
        "(Compare your results to the state-of-the-art performance)\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Bonus Task (optional)\n",
        "\n",
        "### 5.1. Annotating out-of-domain documents\n",
        "\n",
        "(Briefly describe the chosen out-of-domain documents)\n",
        "\n",
        "(Briefly describe the process of annotation)\n",
        "\n",
        "### 5.2 Conversion into dataset"
      ],
      "metadata": {
        "id": "x7ylOS8FdYZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to convert the annotations into a dataset here"
      ],
      "metadata": {
        "id": "32DU04FndRdM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3. Model evaluation on out-of-domain test set"
      ],
      "metadata": {
        "id": "4ghO4JemeFKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to evaluate the model on the out-of-domain test set here"
      ],
      "metadata": {
        "id": "9tzYWQ_zeCYp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Bonus task results\n",
        "\n",
        "(Present the results of the evaluation on the out-of-domain test set)\n",
        "\n",
        "### 5.5. Annotated data"
      ],
      "metadata": {
        "id": "8XLZlItdePfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Include your annotated out-of-domain data here"
      ],
      "metadata": {
        "id": "L2YJsiIGeYRe"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}