{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Introduction-to-Human-Language-Technology/blob/main/course_project_Reza2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to HLT Project (Template)\n",
        "\n",
        "- Student(s) Name(s): Mohammadreza Akhtari\n",
        "- Date: April 2024\n",
        "- Chosen Corpus: imdb\n",
        "- Contributions (if group project): -\n",
        "\n",
        "### Corpus information\n",
        "\n",
        "- Description of the chosen corpus: Large Movie Review Dataset. This is a dataset for binary sentiment classification. A set of 25,000 highly polar movie reviews for training, and 25,000 for testing.\n",
        "- Paper(s) and other published materials related to the corpus: Maas, Andrew, et al. \"Learning word vectors for sentiment analysis.\" Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies. 2011.\n",
        "- State-of-the-art performance (best published results) on this corpus:"
      ],
      "metadata": {
        "id": "ucyWlC5gbOyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 1. Setup"
      ],
      "metadata": {
        "id": "D5d-9uxrcDY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to install and import libraries etc. here\n",
        "!pip install --quiet datasets transformers[torch] evaluate optuna plotly\n",
        "import datasets\n",
        "from pprint import pprint #pprint => pretty-print"
      ],
      "metadata": {
        "id": "caHHQoqEcG1J",
        "outputId": "a8965adf-dbb1-42a6-ab77-27bbebd948ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m837.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Data download and preprocessing\n",
        "\n",
        "### 2.1. Download the corpus"
      ],
      "metadata": {
        "id": "ovUapilSb8iT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PDx40YyzbGPc",
        "outputId": "5ca54222-fc92-49d4-d519-89089fce5bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Your code to download the corpus here¨\n",
        "dataset = datasets.load_dataset(\"stanfordnlp/imdb\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0:2]"
      ],
      "metadata": {
        "id": "oq9WESKtpumj",
        "outputId": "2e776920-7294-4870-ab0d-1baa3670d056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': [\"Before watching this film I had very low expectations and went to just see the cars. Eventually I even regretted going for that reason. Plot is almost non-existent. Character development is non-existent. So many clichés and so much jaw-dropping cheesiness existed in the movie that I could only stare and wonder how it was even released. If not for the exotics, I wouldn't have even rated this movie a 1. An attempt at a coherent story line is destroyed by the sheer absurdity of this elite racing cult and the laughable characters that make up its members. In fact, the movie's plot is so predictable and simple-minded that an average child could foretell the majority of the storyline. Bad acting, bad plot, bad jokes, bad movie.<br /><br />Don't see it. Play Gran Turismo HD instead and it'll satiate your thirst for fast sexy cars without leaving a bad aftertaste.\",\n",
              "  \"I had never heard of this flick despite the connection to George Clooney (whose company produced and he appears in a very funny supporting bit) and his Ocean's 11 director Steven Soderbergh. Worse, we picked this up in a discount bin for $4.99 (Canadian dollars at that!) What a grand and pleasant surprise. But then I'm of the opinion that if William H. Macy is in it you can't be disappointed. This was very reminiscent of those Ealing comedies from England in the 1950s. OK, with more profanity. This is an oddball and at times gut-splittingly funny film. The actual heist made me laugh so hard I was crying. Perhaps the funniest use of underpants in movie history. Maybe it was the low expectations I had going in but I watched with a group of people and we had a blast. Best $5 I've spent in ages.\"],\n",
              " 'label': [0, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Preprocessing"
      ],
      "metadata": {
        "id": "cXb7CQNCbZOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for any necessary preprocessing here\n",
        "#This is never a bad idea, datasets may have ordering to them, which is not what we want\n",
        "dataset=dataset.shuffle()\n",
        "#Delete the unlabeled part of the dataset, we don't need it for anything\n",
        "del dataset[\"unsupervised\"]"
      ],
      "metadata": {
        "id": "RO5BXCuRbYKr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.feature_extraction\n",
        "vectorizer=sklearn.feature_extraction.text.CountVectorizer(binary=True, max_features=25000)\n",
        "\n",
        "#get a list of all texts from the training data\n",
        "texts=[ex[\"text\"] for ex in dataset[\"train\"]]\n",
        "\n",
        "#\"Trains\" the vectorizer, i.e. builds its vocabulary\n",
        "vectorizer.fit(texts)"
      ],
      "metadata": {
        "id": "OHMjKIAYoK4-",
        "outputId": "19fbba4b-fbf0-4116-ba69-3904adb96332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(binary=True, max_features=25000)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True, max_features=25000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True, max_features=25000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_example(ex):\n",
        "    #because the vectorizer expects a list/iterable over inputs, not one input\n",
        "    vectorized=vectorizer.transform([ex[\"text\"]])\n",
        "    #.nonzero gives a pair of (rows,columns), we want the columns\n",
        "    non_zero_features=vectorized.nonzero()[1]\n",
        "    #feature index 0 will have a special meaning\n",
        "    # so let us produce it by adding +1 to everything\n",
        "    non_zero_features+=1\n",
        "    return {\"input_ids\":non_zero_features}\n",
        "\n",
        "vectorized=vectorize_example(dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "X-n6yer9orhq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can map back to vocabulary and check that everything works\n",
        "# vectorizer.vocabulary_ is a dictionary {key:word, value:idx}\n",
        "\n",
        "idx2word=dict((i,w) for (w,i) in vectorizer.vocabulary_.items()) #inverse the vocab dictionary\n",
        "words=[]\n",
        "for idx in vectorized[\"input_ids\"]:\n",
        "    ## It is easy to forgot we moved all by +1\n",
        "    words.append(idx2word[idx-1])\n",
        "#This is now the bag of words representation of the document\n",
        "pprint(\", \".join(words))"
      ],
      "metadata": {
        "id": "HWkg4om7otk7",
        "outputId": "b9b0bea5-4cc6-4f85-c318-d7b3a00858bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('absurdity, acting, aftertaste, almost, an, and, at, attempt, average, bad, '\n",
            " 'before, br, by, cars, character, characters, cheesiness, child, clichés, '\n",
            " 'coherent, could, cult, destroyed, development, don, dropping, elite, even, '\n",
            " 'eventually, existed, existent, expectations, fact, fast, film, for, going, '\n",
            " 'had, have, hd, how, if, in, instead, is, it, its, jaw, jokes, just, '\n",
            " 'laughable, leaving, line, ll, low, majority, make, many, members, minded, '\n",
            " 'movie, much, non, not, of, only, play, plot, predictable, racing, rated, '\n",
            " 'reason, regretted, released, see, sexy, sheer, simple, so, stare, story, '\n",
            " 'storyline, that, the, thirst, this, to, up, very, was, watching, went, '\n",
            " 'without, wonder, wouldn, your')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenizer to the whole dataset using .map()\n",
        "dataset_tokenized = dataset.map(vectorize_example,num_proc=4)\n",
        "pprint(dataset_tokenized[\"train\"][0])"
      ],
      "metadata": {
        "id": "84KNLNkApmNk",
        "outputId": "6eadb844-ce95-47f1-af5f-c1da0cd8431f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [374,\n",
            "               484,\n",
            "               698,\n",
            "               905,\n",
            "               1035,\n",
            "               1066,\n",
            "               1566,\n",
            "               1607,\n",
            "               1722,\n",
            "               1830,\n",
            "               2157,\n",
            "               2840,\n",
            "               3258,\n",
            "               3529,\n",
            "               3779,\n",
            "               3789,\n",
            "               3879,\n",
            "               3934,\n",
            "               4215,\n",
            "               4358,\n",
            "               5123,\n",
            "               5455,\n",
            "               6121,\n",
            "               6177,\n",
            "               6695,\n",
            "               6913,\n",
            "               7295,\n",
            "               7771,\n",
            "               7779,\n",
            "               7918,\n",
            "               7920,\n",
            "               7941,\n",
            "               8099,\n",
            "               8227,\n",
            "               8451,\n",
            "               8767,\n",
            "               9567,\n",
            "               10018,\n",
            "               10262,\n",
            "               10294,\n",
            "               10824,\n",
            "               11010,\n",
            "               11216,\n",
            "               11565,\n",
            "               11851,\n",
            "               11881,\n",
            "               11889,\n",
            "               11969,\n",
            "               12093,\n",
            "               12203,\n",
            "               12701,\n",
            "               12790,\n",
            "               13010,\n",
            "               13093,\n",
            "               13261,\n",
            "               13483,\n",
            "               13486,\n",
            "               13617,\n",
            "               14012,\n",
            "               14219,\n",
            "               14638,\n",
            "               14658,\n",
            "               15164,\n",
            "               15209,\n",
            "               15413,\n",
            "               15511,\n",
            "               16638,\n",
            "               16686,\n",
            "               16998,\n",
            "               17651,\n",
            "               17804,\n",
            "               17897,\n",
            "               18127,\n",
            "               18192,\n",
            "               19589,\n",
            "               19791,\n",
            "               19892,\n",
            "               20181,\n",
            "               20574,\n",
            "               21102,\n",
            "               21309,\n",
            "               21312,\n",
            "               22330,\n",
            "               22335,\n",
            "               22407,\n",
            "               22413,\n",
            "               22599,\n",
            "               23631,\n",
            "               23870,\n",
            "               24232,\n",
            "               24257,\n",
            "               24381,\n",
            "               24629,\n",
            "               24669,\n",
            "               24741,\n",
            "               24906],\n",
            " 'label': 0,\n",
            " 'text': 'Before watching this film I had very low expectations and went to '\n",
            "         'just see the cars. Eventually I even regretted going for that '\n",
            "         'reason. Plot is almost non-existent. Character development is '\n",
            "         'non-existent. So many clichés and so much jaw-dropping cheesiness '\n",
            "         'existed in the movie that I could only stare and wonder how it was '\n",
            "         \"even released. If not for the exotics, I wouldn't have even rated \"\n",
            "         'this movie a 1. An attempt at a coherent story line is destroyed by '\n",
            "         'the sheer absurdity of this elite racing cult and the laughable '\n",
            "         \"characters that make up its members. In fact, the movie's plot is so \"\n",
            "         'predictable and simple-minded that an average child could foretell '\n",
            "         'the majority of the storyline. Bad acting, bad plot, bad jokes, bad '\n",
            "         \"movie.<br /><br />Don't see it. Play Gran Turismo HD instead and \"\n",
            "         \"it'll satiate your thirst for fast sexy cars without leaving a bad \"\n",
            "         'aftertaste.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Machine learning model\n",
        "\n",
        "### 3.1. Model training"
      ],
      "metadata": {
        "id": "F1ntHh_JbrAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to train the machine learning model on the training set and evaluate the performance on the validation set here"
      ],
      "metadata": {
        "id": "Hs2Bf49zbn5C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Hyperparameter optimization"
      ],
      "metadata": {
        "id": "nlO8RVuHcmAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for hyperparameter optimization here"
      ],
      "metadata": {
        "id": "IzDrTDd0cWOG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Evaluation on test set"
      ],
      "metadata": {
        "id": "1EzCYTnfcrvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to evaluate the final model on the test set here"
      ],
      "metadata": {
        "id": "BG7s-yr6crGF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4. Results and summary\n",
        "\n",
        "### 4.1 Corpus insights\n",
        "\n",
        "(Briefly discuss what you learned about the corpus and its annotation)\n",
        "\n",
        "### 4.2 Results\n",
        "\n",
        "(Briefly summarize your results)\n",
        "\n",
        "### 4.3 Relation to state of the art\n",
        "\n",
        "(Compare your results to the state-of-the-art performance)\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Bonus Task (optional)\n",
        "\n",
        "### 5.1. Annotating out-of-domain documents\n",
        "\n",
        "(Briefly describe the chosen out-of-domain documents)\n",
        "\n",
        "(Briefly describe the process of annotation)\n",
        "\n",
        "### 5.2 Conversion into dataset"
      ],
      "metadata": {
        "id": "x7ylOS8FdYZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to convert the annotations into a dataset here"
      ],
      "metadata": {
        "id": "32DU04FndRdM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3. Model evaluation on out-of-domain test set"
      ],
      "metadata": {
        "id": "4ghO4JemeFKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code to evaluate the model on the out-of-domain test set here"
      ],
      "metadata": {
        "id": "9tzYWQ_zeCYp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Bonus task results\n",
        "\n",
        "(Present the results of the evaluation on the out-of-domain test set)\n",
        "\n",
        "### 5.5. Annotated data"
      ],
      "metadata": {
        "id": "8XLZlItdePfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Include your annotated out-of-domain data here"
      ],
      "metadata": {
        "id": "L2YJsiIGeYRe"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}