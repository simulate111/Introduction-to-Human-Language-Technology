{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Introduction-to-Human-Language-Technology/blob/main/sentence_splitting_and_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence splitting and tokenization example\n",
        "\n",
        "This short notebook illustrates one comparatively fast way to do sentence splitting and tokenization in Python. It's not particularly accurate at either, but should do the job in cases where the details don't matter too much.\n",
        "\n",
        "We'll use the [sentence-splitter](https://pypi.org/project/sentence-splitter/) and [regex](https://pypi.org/project/regex/) packages."
      ],
      "metadata": {
        "id": "OZ16vxvAqdKV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FdjkVj9DoO5a",
        "outputId": "70d7de21-7b2c-4aa9-ad77-ca897f98a7fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet sentence-splitter regex"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grab some example data"
      ],
      "metadata": {
        "id": "cVWL_xRdrK6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc http://dl.turkunlp.org/TKO_7095_2023/fiwiki-20221120-sample.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGXFZ2ZNoSTk",
        "outputId": "c9300bf3-f6f1-4435-ff69-ffb6500d300c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-13 19:40:04--  http://dl.turkunlp.org/TKO_7095_2023/fiwiki-20221120-sample.txt\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 500364237 (477M) [text/plain]\n",
            "Saving to: ‘fiwiki-20221120-sample.txt’\n",
            "\n",
            "fiwiki-20221120-sam 100%[===================>] 477.18M  16.8MB/s    in 28s     \n",
            "\n",
            "2024-04-13 19:40:33 (17.2 MB/s) - ‘fiwiki-20221120-sample.txt’ saved [500364237/500364237]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in example data in paragraph-per-line format"
      ],
      "metadata": {
        "id": "tZhPmsM9rOpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = open('fiwiki-20221120-sample.txt').readlines()"
      ],
      "metadata": {
        "id": "fx2506pGoqCW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate sentence splitter. Note that you need to provide the language, and not all languages are supported."
      ],
      "metadata": {
        "id": "7pqTKikyrWKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_splitter import SentenceSplitter\n",
        "\n",
        "splitter = SentenceSplitter(language='fi')"
      ],
      "metadata": {
        "id": "UeK5rcDNoxgM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run sentence splitting and log runtime. Just take some paragraphs from the start to keep things reasonably fast."
      ],
      "metadata": {
        "id": "WEFqCEbLrd_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "sentences = [s for p in paragraphs[:100000] for s in splitter.split(p)]"
      ],
      "metadata": {
        "id": "alhd_yIgpAbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into tokens using a regular expression. Here the regular expression defines as a token any sequence of alphanumeric characters or any (other) single non-space character."
      ],
      "metadata": {
        "id": "xN6m8TE_riXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex\n",
        "\n",
        "TOKENIZE_RE = regex.compile(r'([[:alnum:]]+|\\S)')"
      ],
      "metadata": {
        "id": "t9V-nxRIpGxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize and log runtime"
      ],
      "metadata": {
        "id": "7i9iVu8wrwoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tokenized = [TOKENIZE_RE.findall(s) for s in sentences]"
      ],
      "metadata": {
        "id": "h9pRUUYDp3w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check a few examples"
      ],
      "metadata": {
        "id": "nCscgzWsr5oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t in tokenized[:10]:\n",
        "    print(t)"
      ],
      "metadata": {
        "id": "LmUGB8eDqAyl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}