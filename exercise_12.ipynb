{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Introduction-to-Human-Language-Technology/blob/main/exercise_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 12: analogy evaluation\n",
        "\n",
        "In the lecture, we touched upon *Mikolov's analogy dataset* which was one of the first analogy evaluation datasets for word embeddings. It consists of 9+5=14 sets of word analogies. You can find it for example here: https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt\n",
        "\n",
        "It might be interesting to know how well our embeddings fare on each of these 14 tasks. And that will be our exercise. The steps are as follows:\n",
        "\n",
        "1. Read in the analogy tuples from the file above, for each task separately (the format of the file is kinda self-explanatory once you open it)\n",
        "2. Write a function `eval_analogy(tuples,embeddings,K)` which will return the top-K accuracy of the `embeddings` (Gensim's KeyedVectors) on `tuples`, which are the analogy 4-tuples. For instance for the tuple (\"Athens\",\"Greece\",\"Havana\",\"Cuba\") will be counted as correct if the analogy on the first three words results in K nearest neighbors that contain also \"Cuba\". Hope this is clear. :)\n",
        "3. Run this function on the 14 tasks you read in step (1) and see if you see any interesting differences\n",
        "\n",
        "Below is the relevant embedding-loading and analogy example code from the lecture that you can reuse.\n",
        "\n",
        "**Tip:** these do take a while to compute, so you might want to debug your code on a small sample and when happy, run the whole thing only once. I also like to use `tqdm` to get a progress bar, so I see how long I need to wait to see some output."
      ],
      "metadata": {
        "id": "AeYKb5I5bAeJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c3-FPp5MZjAS"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I found this link in the NLPL repository\n",
        "# It refers to English model trained on the Gigaword corpus of news\n",
        "##!wget http://vectors.nlpl.eu/repository/20/12.zip\n",
        "\n",
        "## Try these if the download above is too slow, I mirrored these:\n",
        "!wget http://dl.turkunlp.org/TKO_7095_2023/12.zip\n",
        "#!wget http://dl.turkunlp.org/TKO_7095_2023/42.zip\n"
      ],
      "metadata": {
        "id": "QRbTPYZGZuiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7469c3bc-12f1-4ca5-9495-0744e28043e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-22 19:55:40--  http://dl.turkunlp.org/TKO_7095_2023/12.zip\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 613577258 (585M) [application/zip]\n",
            "Saving to: ‘12.zip’\n",
            "\n",
            "12.zip              100%[===================>] 585.15M  20.3MB/s    in 30s     \n",
            "\n",
            "2024-04-22 19:56:11 (19.2 MB/s) - ‘12.zip’ saved [613577258/613577258]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Somewhat awkwardly, these are numbered files and both\n",
        "# .zip files contain \"model.bin\"\n",
        "# Let's unzip and rename\n",
        "# -o means \"do not ask, overwrite by default\"\n",
        "!unzip -o 12.zip\n",
        "!mv model.bin en.bin\n"
      ],
      "metadata": {
        "id": "AiV_3crkzBtI",
        "outputId": "b4b1308b-5cdd-429d-be16-5f5bf3537532",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  12.zip\n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Now we can load the embeddings\n",
        "*   These are huge, but they are sorted by frequency, so we can easily limit ourselves to the top 100,000 words, which will be plenty enough for us\n",
        "*   This is maybe good to note, now we enter the territory of NLP models which count in the gigabytes in size\n",
        "\n"
      ],
      "metadata": {
        "id": "lL1rXODZOCYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is how you load the trained embeddings\n",
        "# check the documentation\n",
        "# w2v embeddings are traditionlly distributed in one of two formats: a text form, and a binary form\n",
        "# The embeddings we downloaded above are in the binary form, so we need to indicate that when loading\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "wv_emb_en=KeyedVectors.load_word2vec_format(\"en.bin\", limit=100000, binary=True)\n"
      ],
      "metadata": {
        "id": "4DbFt1VOaDCu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`KeyedVectors` documentation is here: https://radimrehurek.com/gensim/models/keyedvectors.html"
      ],
      "metadata": {
        "id": "mEsD37OxeP2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic operations with the embeddings\n",
        "\n",
        "* The KeyedVectors object allows for all the basic operations with embeddings which we saw in the lecture\n"
      ],
      "metadata": {
        "id": "FT9CdAlCOW3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word analogy\n",
        "\n",
        "* \"A is to B as C is to D\"\n",
        "* Can be implemented as D=B-A+C, where (A,B,C) are word embeddings\n",
        "* Then list words nearest to the computed embedding D\n",
        "* In the library, the implementation lets us list words with \"+\" sign, and words with \"-\" sign\n"
      ],
      "metadata": {
        "id": "XtyrScWuOokr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# B     A      C\n",
        "# Paris-France+Sweden= ___?\n",
        "#\n",
        "# i.e. France is to Paris as Sweden is to X\n",
        "wv_emb_en.most_similar(positive=[\"Paris\",\"Sweden\"],negative=[\"France\"])"
      ],
      "metadata": {
        "id": "7PaVLPxsOujq",
        "outputId": "29e657fb-00c5-42f2-e64c-04ab0a78887e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Stockholm', 0.7338932752609253),\n",
              " ('Malmo', 0.5458161234855652),\n",
              " ('Helsinki', 0.5444940328598022),\n",
              " ('Goteborg', 0.5421050190925598),\n",
              " ('Swedish', 0.5309098362922668),\n",
              " ('Malmoe', 0.5198634266853333),\n",
              " ('Oslo', 0.5004472732543945),\n",
              " ('Gothenburg', 0.4957912266254425),\n",
              " ('STOCKHOLM', 0.48791587352752686),\n",
              " ('Copenhagen', 0.47769418358802795)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "triples=[(\"cow\",\"milk\",\"hen\"),\n",
        "         (\"Paris\",\"France\",\"Helsinki\"),\n",
        "         (\"car\",\"wheel\",\"airplane\"),\n",
        "         (\"airplane\",\"propeller\",\"ship\"),\n",
        "         (\"king\",\"queen\",\"man\"),\n",
        "         (\"man\",\"doctor\",\"woman\"),\n",
        "         (\"man\",\"boss\",\"woman\")\n",
        "         ]\n",
        "for what,is_to_what,as_this_is in triples:\n",
        "    # is_to_what-what+as_this_is\n",
        "    to_what=wv_emb_en.most_similar(positive=[is_to_what,as_this_is],negative=[what])[0][0]\n",
        "    print(f\"{what} is to {is_to_what} as {as_this_is} is to: {to_what}\")\n"
      ],
      "metadata": {
        "id": "-85ZWiCmPVv9",
        "outputId": "c5cdc419-3795-463e-e740-68ac4b2cfb2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cow is to milk as hen is to: sauce\n",
            "Paris is to France as Helsinki is to: Finland\n",
            "car is to wheel as airplane is to: rudder\n",
            "airplane is to propeller as ship is to: vessel\n",
            "king is to queen as man is to: woman\n",
            "man is to doctor as woman is to: physician\n",
            "man is to boss as woman is to: bosses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The exercise code starts below\n",
        "\n",
        "* I will donate you a function for reading Mikolov's data, but I recommend you delete it and write your own as a further exercise\n",
        "* Reading annoying file formats is an integral part of NLP"
      ],
      "metadata": {
        "id": "BuMn_21DlLTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remember you always need to download the \"raw\" link from GitHub, or else you get an HTML with the pretty-printed data, not the data itself\n",
        "!wget https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt"
      ],
      "metadata": {
        "id": "CBx6TuWtliB-",
        "outputId": "2607c721-c4c6-4117-f54e-d2551e545162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-22 19:56:28--  https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 603955 (590K) [text/plain]\n",
            "Saving to: ‘questions-words.txt’\n",
            "\n",
            "\rquestions-words.txt   0%[                    ]       0  --.-KB/s               \rquestions-words.txt 100%[===================>] 589.80K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-04-22 19:56:28 (13.7 MB/s) - ‘questions-words.txt’ saved [603955/603955]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tasks={} #A dictionary with taskname as key, and value will then be a list of 4-tuples with the analogy data\n",
        "\n",
        "with open(\"questions-words.txt\") as f:\n",
        "    for line in f:\n",
        "        line=line.rstrip(\"\\n\")\n",
        "        if not line:\n",
        "            continue #skip possible empty lines\n",
        "        if line.startswith(\": \"): #All tasks seem to start with a line like \": task-name\"\n",
        "            taskname=line[2:] #get rid of \": \"\n",
        "            tuple_list=[] #let's make a new list for the tuples and store it into the tasks dictionary\n",
        "            #then we keep filling it, until a new task starts, when a new list is created, the previous\n",
        "            #of course remains in the `tasks` dictionary\n",
        "            tasks[taskname]=tuple_list\n",
        "        else: #not a task line, so this should be a 4-word line, with words space-separated it seems\n",
        "            w1,w2,w3,w4=line.split()\n",
        "            tuple_list.append((w1,w2,w3,w4)) #let's append it and we're done\n",
        "\n",
        "print(f\"We have {len(tasks)} tasks.\")\n"
      ],
      "metadata": {
        "id": "iT4EFRMUlwoS",
        "outputId": "ef1ce8ff-8e3f-4f16-8516-a40d0a7d9f19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 14 tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_analogy(tuples, embeddings, K):\n",
        "  \"\"\"\n",
        "  This function evaluates the accuracy of the embeddings on analogy tasks.\n",
        "\n",
        "  Args:\n",
        "      tuples: A list of 4-tuples representing analogy problems (word1, word2, word3, word4).\n",
        "      embeddings: A Gensim KeyedVectors object containing word embeddings.\n",
        "      K: The number of nearest neighbors to consider for the analogy.\n",
        "\n",
        "  Returns:\n",
        "      A float representing the accuracy of the embeddings on the analogy tasks.\n",
        "  \"\"\"\n",
        "\n",
        "  correct = 0\n",
        "  total = len(tuples)\n",
        "\n",
        "  for w1, w2, w3, w4 in tqdm.tqdm(tuples):\n",
        "    # Calculate the analogy vector: w2 - w1 + w3\n",
        "    analogy_vec = embeddings[w2] - embeddings[w1] + embeddings[w3]\n",
        "\n",
        "    # Find the K nearest neighbors of the analogy vector\n",
        "    nearest_neighbors = embeddings.most_similar(positive=[analogy_vec], topn=K)\n",
        "\n",
        "    # Check if the correct word (w4) is among the nearest neighbors\n",
        "    if any(neighbor[0] == w4 for neighbor in nearest_neighbors):\n",
        "      correct += 1\n",
        "\n",
        "  # Calculate and return the accuracy\n",
        "  accuracy = correct / total\n",
        "  return accuracy\n"
      ],
      "metadata": {
        "id": "Yi8cgF0yePdy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "def eval_analogy(tuples, embeddings, K):\n",
        "    count = 0\n",
        "    total = 0\n",
        "    for analogy in tqdm.tqdm(tuples):\n",
        "        if all(word in embeddings.key_to_index for word in analogy):\n",
        "            Analogy = embeddings.most_similar(positive=[analogy[1], analogy[2]], negative=[analogy[0]], topn=K)\n",
        "            predict = [word for word, _ in Analogy]\n",
        "            if analogy[3] in predict:\n",
        "                count += 1\n",
        "            total += 1\n",
        "    accuracy = count / total * 100 if total > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "### MY results are\n",
        "# Task *gram9-plural-verbs* has top-3 accuracy of 83.45%\n",
        "# Task *capital-common-countries* has top-3 accuracy of 93.68%\n",
        "# Task *capital-world* has top-3 accuracy of 95.97%\n",
        "# Task *currency* has top-3 accuracy of 40.54%\n",
        "# Task *city-in-state* has top-3 accuracy of 63.76%\n",
        "# Task *family* has top-3 accuracy of 93.16%\n",
        "# Task *gram1-adjective-to-adverb* has top-3 accuracy of 49.25%\n",
        "# Task *gram2-opposite* has top-3 accuracy of 49.62%\n",
        "# Task *gram3-comparative* has top-3 accuracy of 95.50%\n",
        "# Task *gram4-superlative* has top-3 accuracy of 86.32%\n",
        "# Task *gram5-present-participle* has top-3 accuracy of 83.97%\n",
        "# Task *gram6-nationality-adjective* has top-3 accuracy of 95.45%\n",
        "# Task *gram7-past-tense* has top-3 accuracy of 90.45%\n",
        "# Task *gram8-plural* has top-3 accuracy of 89.04%\n",
        "# Task *gram9-plural-verbs* has top-3 accuracy of 83.45%"
      ],
      "metadata": {
        "id": "ZkD_Ky24nvqn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task, analogy in tasks.items():\n",
        "    accuracy = eval_analogy(analogy, wv_emb_en, K=3)\n",
        "    print(f\"{task}: {accuracy}%\")"
      ],
      "metadata": {
        "id": "7VT7sbGSgVoY",
        "outputId": "809541a8-a090-4530-e6d0-93b91afb6379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 506/506 [00:11<00:00, 44.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capital-common-countries: 93.67588932806325%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4524/4524 [01:15<00:00, 60.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capital-world: 95.96520898767818%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 866/866 [00:07<00:00, 110.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "currency: 56.11353711790393%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2467/2467 [00:49<00:00, 50.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "city-in-state: 63.76165383056344%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 506/506 [00:08<00:00, 57.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "family: 93.15789473684211%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 992/992 [00:18<00:00, 52.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gram1-adjective-to-adverb: 49.24731182795699%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 812/812 [00:12<00:00, 64.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gram2-opposite: 51.455026455026456%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1332/1332 [00:23<00:00, 56.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gram3-comparative: 95.4954954954955%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1122/1122 [00:19<00:00, 56.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gram4-superlative: 89.01515151515152%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1056/1056 [00:16<00:00, 62.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gram5-present-participle: 83.97177419354838%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1599/1599 [00:29<00:00, 55.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gram6-nationality-adjective: 97.89612097304405%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1560/1560 [00:30<00:00, 51.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gram7-past-tense: 90.44871794871796%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1332/1332 [00:23<00:00, 56.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gram8-plural: 89.03903903903904%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 870/870 [00:16<00:00, 53.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gram9-plural-verbs: 83.44827586206897%\n"
          ]
        }
      ]
    }
  ]
}